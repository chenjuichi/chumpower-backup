import os
import datetime
import pathlib
import csv
import time
import shutil
import psutil
import glob
import math

import pymysql
from sqlalchemy import exc
from sqlalchemy import exists

import re

import pandas as pd

import openpyxl
from openpyxl import Workbook, load_workbook
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.cell.rich_text import CellRichText, TextBlock, InlineFont
from openpyxl.utils import get_column_letter

from database.tables import User, Session, Material, Bom, Assemble, Product
from database.p_tables import P_Material, P_Bom, P_Assemble, P_Product, P_Part

from database.tables import ProcessedFile
from flask import Blueprint, jsonify, request, current_app

import warnings
warnings.filterwarnings(
   "ignore",
   category=FutureWarning,
  message=".*Setting an item of incompatible dtype.*"
)

excelTable = Blueprint('excelTable', __name__)

from log_util import setup_logger
logger = setup_logger(__name__)                 # æ¯å€‹æ¨¡çµ„ç”¨è‡ªå·±çš„åç¨±


# ------------------------------------------------------------------


def pick_order_col(df, candidates=("è¨‚å–®", "å–®è™Ÿ", "å·¥å–®", "å–®æ“š")):
    # å…ˆæ¸…æ¬„ä½åï¼ˆå»ç©ºç™½ã€æ›è¡Œï¼‰
    df.columns = [str(c).strip() for c in df.columns]

    for c in candidates:
        if c in df.columns:
            return c

    # æ‰¾ä¸åˆ°å°±é€€å›ç¬¬ä¸€æ¬„ï¼ˆé€šå¸¸ç¬¬ä¸€æ¬„å°±æ˜¯è¨‚å–®/å–®è™Ÿï¼‰
    return df.columns[0] if len(df.columns) > 0 else None


def read_all_excel_process_code_p():
    """
    å¾ _server_dir ç›®éŒ„è£¡çš„ Excel æª”æ¡ˆï¼ˆåªè™•ç† .xlsx/.xlsmï¼‰ï¼Œ
    è®€å–å·¥ä½œè¡¨ã€Œé…ä»¶å·¥ä½œä¸­å¿ƒè³‡æ–™è¡¨-0922 (2)ã€ä¸­
    æ¬„ä½ã€Œè£½ç¨‹ä»£è™Ÿ \n(æ¨™æº–å…§æ–‡ç¢¼)ã€çš„å…§å®¹ï¼Œ

    è‹¥è©²æ¬„ä½å…±æœ‰ m ç­†æœ‰æ•ˆè³‡æ–™ï¼š
      ç¬¬ 1 åˆ— -> m
      ç¬¬ 2 åˆ— -> m-1
      ...
      ç¬¬ m åˆ— -> 1

    ç”¢ç”Ÿä¸¦å›å‚³ code_to_assembleStep = { '100-01': m, '100-02': m-1, ... }
    ï¼ˆæ³¨æ„ï¼šé€™è£¡æœƒæŠŠå‰é¢çš„ 'B' å»æ‰ï¼Œå› ç‚ºå¾Œé¢ä½ æœ‰ code = workNum[1:]ï¼‰
    """

    _base_dir = current_app.config['baseDir']
    _server_dir = _base_dir.replace("excel_in", "server")

    #print("read_all_excel_process_code_p(), _base_dir:", _base_dir)
    #print("read_all_excel_process_code_p(), _server_dir:", _server_dir)

    code_to_assembleStep = {}

    target_sheet_name = "é…ä»¶å·¥ä½œä¸­å¿ƒè³‡æ–™è¡¨-0922 (2)"
    # å¯èƒ½çš„æ¬„ä½åç¨±
    target_col_candidates = [
      "è£½ç¨‹ä»£è™Ÿ \n(æ¨™æº–å…§æ–‡ç¢¼)",
      "è£½ç¨‹ä»£è™Ÿ (æ¨™æº–å…§æ–‡ç¢¼)",
    ]

    # åªæƒæ openpyxl æ”¯æ´çš„æ ¼å¼
    valid_exts = (".xlsx", ".xlsm", ".xltx", ".xltm")

    for root, dirs, files in os.walk(_server_dir):
        for fname in files:
            lower = fname.lower()

            # 1) å…ˆéæ¿¾æ‰é Excel æª”æ¡ˆï¼ˆä¸å†å»é–‹ .bat / .pyï¼‰
            if not lower.endswith(valid_exts) and not lower.endswith(".xls"):
              continue

            full_path = os.path.join(root, fname)
            #print(f"è™•ç† Excel æª”æ¡ˆ: {full_path}")

            # 2) .xls ç›´æ¥æç¤ºï¼šopenpyxl ä¸æ”¯æ´ï¼Œè¦ä½ æ”¹æˆ .xlsx
            if lower.endswith(".xls"):
              print(f"  æª”æ¡ˆ {fname} ç‚ºèˆŠç‰ˆ .xlsï¼Œopenpyxl ä¸æ”¯æ´ï¼Œè«‹å¦å­˜ç‚º .xlsx å¾Œå†ä½¿ç”¨ã€‚")
              continue

            # 3) å…¶ä»–å‰¯æª”åï¼ˆ.xlsx/.xlsmâ€¦ï¼‰æ‰ç”¨ openpyxl è®€
            try:
              wb = load_workbook(full_path, data_only=True)
            except Exception as e:
              print(f"  è¼‰å…¥å·¥ä½œç°¿å¤±æ•—: {e}")
              continue

            if target_sheet_name not in wb.sheetnames:
              print(f"  æª”æ¡ˆ {fname} æ²’æœ‰å·¥ä½œè¡¨ã€Œ{target_sheet_name}ã€ï¼Œç•¥éã€‚")
              continue

            ws = wb[target_sheet_name]

            # è®€ç¬¬ä¸€åˆ—ç•¶è¡¨é ­
            header_row = next(ws.iter_rows(min_row=1, max_row=1, values_only=True), None)
            if not header_row:
              print(f"  æª”æ¡ˆ {fname} çš„è¡¨é ­æ˜¯ç©ºçš„ï¼Œç•¥éã€‚")
              continue

            # æ‰¾ç›®æ¨™æ¬„ä½çš„ column indexï¼ˆ1-basedï¼‰
            target_col_idx = None

            # å…ˆç›´æ¥æ¯”å°å€™é¸åç¨±
            for idx, val in enumerate(header_row, start=1):
                if val is None:
                  continue
                val_str = str(val).strip()
                if val_str in target_col_candidates:
                  target_col_idx = idx
                  break

            # æ‰¾ä¸åˆ°å°±æ¨¡ç³Šæœå°‹ã€Œè£½ç¨‹ä»£è™Ÿã€
            if target_col_idx is None:
                for idx, val in enumerate(header_row, start=1):
                  if val is None:
                    continue
                  val_str = str(val).strip()
                  if "è£½ç¨‹ä»£è™Ÿ" in val_str:
                    target_col_idx = idx
                    print(f"  ç™¼ç¾ç›¸ä¼¼æ¬„ä½åç¨±ä½¿ç”¨: {val_str} (col {idx})")
                    break

            if target_col_idx is None:
                print(f"  æª”æ¡ˆ {fname} æ‰¾ä¸åˆ°ã€Œè£½ç¨‹ä»£è™Ÿã€æ¬„ä½ï¼Œç•¥éã€‚è¡¨é ­: {list(header_row)}")
                continue

            # å–å‡ºè©²æ¬„ä½å¾ç¬¬2åˆ—é–‹å§‹çš„æ‰€æœ‰éç©ºå€¼
            codes = []
            for row in ws.iter_rows(min_row=2, values_only=True):
                if target_col_idx - 1 >= len(row):
                  continue
                cell_val = row[target_col_idx - 1]
                if cell_val is None:
                  continue
                cell_str = str(cell_val).strip()
                if cell_str == "":
                  continue

                # çµ±ä¸€æŠŠå‰é¢çš„ 'B' å»æ‰ï¼Œè®“ key è®Šæˆ '100-01'
                # é€™æ¨£æ‰èƒ½å°ä¸Š read_all_excel_files_p è£¡çš„ code = workNum[1:]
                if cell_str.startswith("B"):
                  cell_str = cell_str[1:]

                codes.append(cell_str)

            if not codes:
              print(f"  æª”æ¡ˆ {fname} çš„ç›®æ¨™æ¬„ä½æ²’æœ‰æœ‰æ•ˆè³‡æ–™ï¼Œç•¥éã€‚")
              continue

            m = len(codes)
            print(f"  æª”æ¡ˆ {fname} æœ‰ {m} ç­†è£½ç¨‹ä»£è™Ÿã€‚")

            # ç¬¬1åˆ— -> m, ç¬¬2åˆ— -> m-1, ..., ç¬¬måˆ— -> 1
            for idx, code in enumerate(codes, start=1):
              value = m - idx + 1
              code_to_assembleStep[code] = value
              # print(f"    {idx}-th row, code={code}, value={value}")

    print("read_all_excel_process_code_p(), å®Œæˆï¼Œç¸½ç­†æ•¸:", len(code_to_assembleStep))
    return code_to_assembleStep


# å®‰å…¨è½‰æ•´æ•¸çš„å·¥å…·
def to_int0(v):
    if v is None:
        return 0
    s = str(v).strip()
    if s == '' or s.lower() == 'nan':
        return 0
    try:
        return int(float(s))
    except Exception:
        return 0


# ç”Ÿæˆå”¯ä¸€æª”æ¡ˆåç¨±çš„å‡½å¼
def get_unique_filename(target_dir, filename, chip):
    base, ext = os.path.splitext(filename)  # åˆ†é›¢æª”æ¡ˆåç¨±èˆ‡å‰¯æª”å
    counter = 1
    unique_filename = filename
    while os.path.exists(os.path.join(target_dir, unique_filename)):  # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
        unique_filename = f"{base}_{chip}_{counter}{ext}"  # ç‚ºæª”åæ–°å¢å¾Œç¶´
        counter += 1
    return unique_filename


#to convert date format
def convert_date(date_value):
    try:
        return pd.to_datetime(date_value).date()  # ä½¿ç”¨ pandas è½‰æ›ç‚ºæ—¥æœŸ
    except (ValueError, TypeError):
        return None


def close_open_file(filepath):
    # éæ­·æ‰€æœ‰é€²ç¨‹
    for proc in psutil.process_iter(['pid', 'name']):
        try:
            # æª¢æŸ¥è©²é€²ç¨‹æ‰“é–‹çš„æ‰€æœ‰æ–‡ä»¶
            for item in proc.open_files():
                if item.path == filepath:
                    # å¦‚æœæ–‡ä»¶è¢«æ‰“é–‹ï¼Œå˜—è©¦é—œé–‰
                    print(f"é—œé–‰é€²ç¨‹ {proc.pid} æ‰“é–‹çš„æ–‡ä»¶: {filepath}")
                    proc.terminate()  # çµ‚æ­¢é€²ç¨‹
                    proc.wait()  # ç­‰å¾…é€²ç¨‹çµ‚æ­¢
                    return True
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            continue
    return False


def pad_zeros(str):
  return str.zfill(4)


def clean_nan(value):
    """æ¸…ç†å–®ä¸€å€¼çš„ NaN / NaTï¼Œè½‰ç‚º None"""
    if pd.isna(value) or str(value).lower() == 'nan':
        return None
    return value


def norm_col(c):
    # å»æ‰æ‰€æœ‰ç©ºç™½(å« \n \t) + å»é ­å°¾
    return re.sub(r"\s+", "", str(c)).strip()


def dedupe_columns(cols):
    seen = {}
    new_cols = []
    for c in cols:
        if c not in seen:
            seen[c] = 0
            new_cols.append(c)
        else:
            seen[c] += 1
            new_cols.append(f"{c}.{seen[c]}")  # ç¬¬äºŒå€‹åŒå => .1, ç¬¬ä¸‰å€‹ => .2
    return new_cols


def normalize_order_number(value):
    """
    å°‡å–®è™Ÿè½‰ç‚ºç´”å­—ä¸²ï¼Œä¸å¸¶å°æ•¸é»ï¼Œä¸¦è™•ç† NaNã€‚
    ä¾‹å¦‚ï¼š121100017702.0 -> '121100017702'
    """
    if pd.isna(value) or str(value).lower() == 'nan':
        return ''
    try:
        return str(int(float(value)))
    except (ValueError, TypeError):
        return str(value).strip()


@excelTable.route("/fetchGlobalVar", methods=['GET'])
def fetch_global_var():
  print("fetchGlobalVar....")

  global global_var
  return jsonify({'value': global_var})


@excelTable.route("/countExcelFilesP", methods=['GET'])
def count_excel_files_p():
    print("countExcelFilesP....")

    _base_dir = current_app.config['baseDir']
    _target_dir = _base_dir.replace("_in", "_out")
    _base_dir = _base_dir.replace("_in", "_in_p")
    print("read excel files, ç›®éŒ„: ", _base_dir)
    print("move excel files to, ç›®éŒ„: ", _target_dir)

    path_pattern = f"{_base_dir}/*.xlsx"

    files = glob.glob(path_pattern)         # æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„æª”æ¡ˆ
    count = len(files)                      # è¨ˆç®—æª”æ¡ˆæ•¸é‡
    print("count:", count)

    return jsonify({
      'count': count
    })


@excelTable.route("/countExcelFiles", methods=['GET'])
def count_excel_files():
    print("countExcelFiles....")

    _base_dir = current_app.config['baseDir']

    # æ§‹å»ºè·¯å¾‘æ¨¡å¼ï¼ŒåŒ¹é…ä»¥ "Report_" é–‹é ­çš„ .xlsx æª”æ¡ˆ
    #path_pattern = f"{_base_dir}/Report_*.xlsx"
    path_pattern = f"{_base_dir}/*.xlsx"
    # ä½¿ç”¨ glob æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„æª”æ¡ˆ
    files = glob.glob(path_pattern)
    # è¨ˆç®—æª”æ¡ˆæ•¸é‡
    count = len(files)
    print("count:", count)
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°æª”æ¡ˆï¼Œè¿”å›0
    #return count if count > 0 else 0
    return jsonify({
      'count': count
    })


@excelTable.route("/readAllExcelFilesP", methods=['GET'])
def read_all_excel_files_p():
  print("readAllExcelFilesP....")

  global global_var_p

  return_value = False
  return_message1 = 'éŒ¯èª¤, æ²’æœ‰å·¥å–®æª”æ¡ˆ!'
  file_count_total = 0  #æª”æ¡ˆç¸½æ•¸

  _base_dir = current_app.config['baseDir']
  _target_dir = _base_dir.replace("_in", "_out")
  _base_dir = _base_dir.replace("_in", "_in_p")
  #print("read excel files, ç›®éŒ„: ", _base_dir)
  #print("move excel files to, ç›®éŒ„: ", _target_dir)

  # è®€å–æŒ‡å®šç›®éŒ„ä¸‹çš„æ‰€æœ‰æŒ‡å®šæª”æ¡ˆåç¨±
  files = [
     f for f in os.listdir(_base_dir)
     if os.path.isfile(os.path.join(_base_dir, f))
     and f.endswith('.xlsx')
  ]

  if not files:
    return jsonify({'status': False, 'message': 'éŒ¯èª¤, æ²’æœ‰å·¥å–®æª”æ¡ˆ!'})

  sheet_names_to_check = [
    current_app.config['p_excel_product_sheet'],
    current_app.config['excel_bom_sheet'],
    current_app.config['excel_work_time_sheet']
  ]

  s = Session()

  part_info_map = {}
  for p in s.query(P_Part).all():
      code = (p.part_code or '').strip()
      if not code:
          continue
      part_info_map[code] = {
          'comment': (p.part_comment or '').strip(),
          'process_step_code': int(p.process_step_code or 0)
      }

  for _file_name in files:  #æª”æ¡ˆè®€å–, for loop_1
    file_count_total +=1
    file_name_base = re.sub(r'_copy_\d+', '', _file_name)   # å¾æª”åä¸­ç§»é™¤åƒ _copy_1ã€_copy_2 é€™æ¨£çš„å­—ä¸²ï¼Œå–å¾—åŸå§‹æª”æ¡ˆåç¨±

    # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
    already_processed = s.query(
      exists().where(ProcessedFile.file_name == file_name_base)
    ).scalar()

    if already_processed:
      return_message1 = f"æª”æ¡ˆ {_file_name} å·²è™•ç†éï¼Œ è«‹å†é‡æ•´ç€è¦½å™¨!"
      print(return_message1)
      _path = os.path.join(_base_dir, _file_name)
      file_count_total -=1
    else:
      _path = _base_dir + '\\' + _file_name
      global_var = _path + ' æª”æ¡ˆè®€å–ä¸­...'

      with open(_path, 'rb') as file:   # with loop_1_a
        workbook = openpyxl.load_workbook(filename=file, read_only=True)
        return_value = True
        return_message1 = ''

        print("workbook.sheetnames:", workbook.sheetnames)

        missing_sheets = [
          sheet for sheet in sheet_names_to_check if sheet not in workbook.sheetnames
        ]

        if missing_sheets:
          return jsonify({'status': False, 'message': 'éŒ¯èª¤, å·¥å–®æª”æ¡ˆå…§æ²’æœ‰ç›¸é—œå·¥ä½œè¡¨!'})

        print(sheet_names_to_check[0] + ' sheet exists, data reading...')

        material_df = pd.read_excel(_path, sheet_name=0)  # First sheet for Material
        material_df.rename(columns={c: norm_col(c) for c in material_df.columns}, inplace=True)
        material_df.columns = dedupe_columns(material_df.columns)

        bom_df = pd.read_excel(_path, sheet_name=1).fillna('')

        assemble_df = pd.read_excel(_path, sheet_name=2).fillna('')

        print("columns: material_df")
        print(list(material_df.columns))
        print("columns: bom_df")
        print(list(bom_df.columns))
        print("columns: assemble_df")
        print(list(assemble_df.columns))

        # è™•ç† BOM å’Œ Assemble çš„ è¨‚å–®æ¬„ä½
        if 'è¨‚å–®' in bom_df.columns:
          bom_df.iloc[:, 0] = (
            bom_df.iloc[:, 0]
            .apply(normalize_order_number)
            .replace('nan', '')
            .astype(str)
          )
          bom_df = bom_df.astype({bom_df.columns[0]: object})   # æŠŠ dtype è¨­æˆ object
        else:
          # å¦‚æœæ²’è³‡æ–™ï¼Œå»ºç«‹ä¸€å€‹ç©ºçš„ DataFrameï¼Œæˆ–ç›´æ¥è·³é
          bom_df = pd.DataFrame(columns=["è¨‚å–®"])  # é è¨­ç©ºçµæ§‹
          print("âš ï¸ bom_df æ²’æœ‰è³‡æ–™æˆ–ç¼ºå°‘ 'è¨‚å–®' æ¬„ä½ï¼Œå·²å»ºç«‹ç©º DataFrame")
        # end if

        print("bom_df:", bom_df, len(bom_df) )

        if 'è¨‚å–®' in assemble_df.columns:
          assemble_df['è¨‚å–®'] = (
            assemble_df['è¨‚å–®']
            .apply(normalize_order_number)
            .replace('nan', '')
            .astype(str)
          )
          assemble_df = assemble_df.astype({'è¨‚å–®': object})
        else:
          # å¦‚æœæ²’æœ‰è¨‚å–®æ¬„ä½ï¼Œå°±å°å‡ºè­¦å‘Šï¼Œå¾Œé¢å°±ä¸€å®šæŠ“ä¸åˆ°è³‡æ–™
          print("âš ï¸ assemble_df ç¼ºå°‘ 'è¨‚å–®' æ¬„ä½ï¼Œç„¡æ³•ä¾è¨‚å–®éæ¿¾å·¥åºè³‡æ–™! columns =", list(assemble_df.columns))

        # å…ˆè¨˜éŒ„æ•´å€‹ BOM sheet æ˜¯å¦å®Œå…¨æ²’è³‡æ–™
        ##bom_df_is_empty = bom_df.empty

        # åŒå·¥å–®åˆä½µæµç¨‹

        # å–®è™Ÿæ¬„ä½
        material_df["å–®è™Ÿ"] = material_df["å–®è™Ÿ"].apply(normalize_order_number)

        time_cols = [
            "B100åŠ å·¥(ä¸€)å·¥æ™‚(åˆ†)", "B100åŠ å·¥(ä¸€)å·¥æ™‚(åˆ†).1",
            "B102KLåŠ å·¥ç¶œåˆå·¥æ™‚(åˆ†)", "B102KLåŠ å·¥ç¶œåˆå·¥æ™‚(åˆ†).1",
            "B103KTåŠ å·¥å·¥æ™‚(åˆ†)",     "B103KTåŠ å·¥å·¥æ™‚(åˆ†).1",
            "B107éœ‡å‹•ç ”ç£¨å·¥æ™‚(åˆ†)",    "B107éœ‡å‹•ç ”ç£¨å·¥æ™‚(åˆ†).1",
            "B108ç¶œåˆåŠ å·¥å·¥æ™‚(åˆ†)",    "B108ç¶œåˆåŠ å·¥å·¥æ™‚(åˆ†).1",
        ]

        # åªç•™ä¸‹çœŸçš„å­˜åœ¨çš„æ¬„ä½
        time_cols = [c for c in time_cols if c in material_df.columns]

        for c in time_cols:
          material_df[c] = pd.to_numeric(material_df[c], errors="coerce").fillna(0)

        agg = {c: "sum" for c in time_cols}
        #agg = {c: "max" for c in time_cols}

        # å…¶ä»–æ¬„ä½ä¿ç•™ç¬¬ä¸€ç­†å³å¯
        for c in material_df.columns:
            if c not in agg:
                agg[c] = "first"

        material_df = (
            material_df
            .groupby("å–®è™Ÿ", as_index=False)
            .agg(agg)
        )

        # ï¼ˆé™¤éŒ¯ç”¨ï¼‰
        print("after groupby rows:", len(material_df))

        # è¨˜éŒ„å·²è™•ç†éçš„å–®è™Ÿï¼Œé¿å…é‡è¤‡ç”¢ç”Ÿ P_Material / P_Product / P_Bom / P_Assemble
        #seen_order_nums = set()

        # è™•ç† Material
        for _, row in material_df.iterrows(): # for loop_material
          #order_num = normalize_order_number(row.get('å–®è™Ÿ'))
          order_num = row.get('å–®è™Ÿ')
          if not order_num:
            print(f"[è­¦å‘Š] å–®è™Ÿç‚ºç©ºï¼Œè·³éè©²ç­†è³‡æ–™: {row.to_dict()}")
            continue

          # ğŸ”¹è‹¥é€™å€‹å–®è™Ÿä¹‹å‰è™•ç†éï¼Œå°±ç•¥é
          #if order_num in seen_order_nums:
          #  print(f"[ç•¥é] å–®è™Ÿ {order_num} å·²åœ¨å‰é¢è™•ç†éï¼Œç•¥éé‡è¤‡çš„ Material/BOM/Assemble")
          #  continue

          #seen_order_nums.add(order_num)

          # å…ˆæª¢æŸ¥ BOM æ˜¯å¦æœ‰è³‡æ–™
          if bom_df.empty:
              # æ•´å€‹ BOM sheet æ˜¯ç©ºçš„æƒ…æ³ï¼š
              # ğŸ‘‰ ä¸è¦ç•¥éï¼Œå¾Œé¢æœƒå¹«é€™å€‹å–®è™Ÿå»ºç«‹ä¸€ç­†ã€Œé è¨­ã€çš„ P_Bom
              #bom_entries = None
              bom_entries = pd.DataFrame()
              print(f"[é è¨­] æ•´é«” BOM ç‚ºç©ºï¼Œå–®è™Ÿ {order_num} ä»å»ºç«‹ Material / Product / Assembleï¼Œä¸¦å»º 1 ç­†é è¨­ P_Bomã€‚")
          else:
            bom_entries = bom_df[bom_df['è¨‚å–®'] == order_num]
            #bom_df_is_empty = False
            if bom_entries.empty:
                print(f"[ç•¥é] å–®è™Ÿ {order_num} æ²’æœ‰ BOM è³‡æ–™ï¼Œä¸å»ºç«‹ Material")
                #bom_df_is_empty =True
                ###continue   # ç›´æ¥è·³éï¼Œä¸å»º Material / Product / Assemble

          # data
          # --------- æœ‰ BOM æ‰å»ºç«‹ Material ----------
          tempQty = clean_nan(row.get('æ•¸é‡')) or 0
          temp_sd_time_B100 = float(clean_nan(row.get('B100åŠ å·¥(ä¸€)å·¥æ™‚(åˆ†)')) or 0)
          temp_sd_time_B102 = float(clean_nan(row.get('B102KLåŠ å·¥ç¶œåˆå·¥æ™‚(åˆ†)')) or 0)
          temp_sd_time_B103 = float(clean_nan(row.get('B103KTåŠ å·¥å·¥æ™‚(åˆ†)')) or 0)
          temp_sd_time_B107 = float(clean_nan(row.get('B107éœ‡å‹•ç ”ç£¨å·¥æ™‚(åˆ†)')) or 0)
          temp_sd_time_B108 = float(clean_nan(row.get('B108ç¶œåˆåŠ å·¥å·¥æ™‚(åˆ†)')) or 0)

          print("temp_sd_time_B100: ", "{:.2f}".format(float(temp_sd_time_B100)))
          print("temp_sd_time_B102: ", "{:.2f}".format(float(temp_sd_time_B102)))
          print("temp_sd_time_B103: ", "{:.2f}".format(float(temp_sd_time_B103)))
          print("temp_sd_time_B107: ", "{:.2f}".format(float(temp_sd_time_B107)))
          print("temp_sd_time_B108: ", "{:.2f}".format(float(temp_sd_time_B108)))

          print("order_num: ", order_num)
          print("bom_df.empty: ",bom_df.empty)
          print("bom_entries.empty: ",bom_entries.empty)
          temp_bom_empty = True if bom_df.empty or bom_entries.empty else False

          material_isBom = temp_bom_empty
          material_isTakeOk = temp_bom_empty
          material_isShow = temp_bom_empty

          material = P_Material(
            order_num=order_num,
            material_num=clean_nan(row.get('æ–™è™Ÿ')) or '',
            material_comment=clean_nan(row.get('èªªæ˜')) or '',
            material_qty=tempQty,
            material_date=convert_date(row.get('ç«‹å–®æ—¥')),
            material_delivery_date=convert_date(row.get('äº¤æœŸ')),
            total_delivery_qty=tempQty,
            sd_time_B100="{:.2f}".format(float(temp_sd_time_B100)),
            sd_time_B102="{:.2f}".format(float(temp_sd_time_B102)),
            sd_time_B103="{:.2f}".format(float(temp_sd_time_B103)),
            sd_time_B107="{:.2f}".format(float(temp_sd_time_B107)),
            sd_time_B108="{:.2f}".format(float(temp_sd_time_B108)),
            move_by_automatic_or_manual = False,
            move_by_process_type = 4,
            isBom = material_isBom,
            isTakeOk = material_isTakeOk,
            isShow = material_isShow,
            show1_ok = '2' if material_isBom else '1',
            show2_ok = '3' if material_isBom else '0',
            delivery_qty = tempQty if material_isBom else 0,
          )
          s.add(material)
          s.flush()  # ç¢ºä¿ material.id å¯ç”¨

          # Product
          product = P_Product(material_id=material.id)
          s.add(product)
          s.commit()

          # BOM
          if not temp_bom_empty: # bom_df_is_empty if block
            # ä¸€èˆ¬æƒ…æ³ï¼šæœ‰ BOM è³‡æ–™ï¼Œå°±æŒ‰åŸæœ¬é‚è¼¯ä¾æ“šè¨‚å–®ç·¨è™Ÿå»ºç«‹å¤šç­† P_Bom
            bom_entries = bom_df[bom_df['è¨‚å–®'] == order_num]
            print(f"bom_entries ä¸­çš„è³‡æ–™ç­†æ•¸: {len(bom_entries)}")

            for _, bom_row in bom_entries.iterrows():  # for loop_bom
                shortage = to_int0(bom_row.get('ç‰©æ–™çŸ­ç¼º'))
                bom = P_Bom(
                    material_id=material.id,
                    seq_num=clean_nan(bom_row.get('é ç•™é …ç›®')),
                    material_num=clean_nan(bom_row.get('ç‰©æ–™')),
                    material_comment=clean_nan(bom_row.get('ç‰©æ–™èªªæ˜')),
                    req_qty=clean_nan(bom_row.get('éœ€æ±‚æ•¸é‡')),
                    start_date=convert_date(row.get('äº¤æœŸ')),
                    lack_bom_qty=shortage,
                    receive=(shortage == 0),
                )
                s.add(bom)
            # end for loop_bom
          else:
            # âš ï¸ ç‰¹ä¾‹ï¼šæ•´å€‹ BOM sheet æ˜¯ç©ºçš„ (bom_df: Empty DataFrame)
            # ç‚ºæ¯ä¸€å€‹ Material å»ºç«‹ä¸€ç­†ã€Œé è¨­ã€çš„ P_Bom
            print(f"[é è¨­ BOM] å–®è™Ÿ {order_num} BOM ç‚ºç©ºï¼Œå»ºç«‹ 1 ç­†é è¨­ P_Bom")
            bom = P_Bom(
                material_id=material.id,
                seq_num='',
                material_num='',
                material_comment='é è¨­BOM (åŸå§‹Excelç„¡BOMè³‡æ–™)',
                req_qty=0,   # é€™è£¡å¦‚æœä½ å¸Œæœ›ç­‰æ–¼ tempQty ä¹Ÿå¯ä»¥æ”¹æˆ tempQty
                start_date=convert_date(row.get('äº¤æœŸ')),
                lack_bom_qty=0,
                receive=True,
            )
            s.add(bom)
          # end bom_df_is_empty if block

          s.commit()

          # Assemble
          if 'è¨‚å–®' not in assemble_df.columns:
            print(f"âš ï¸ assemble_df æ²’æœ‰ 'è¨‚å–®' æ¬„ä½ï¼Œç„¡æ³•ç”¢ç”Ÿè©²å·¥å–®çš„ P_Assembleï¼Œorder_num={order_num}")
            assemble_entries = assemble_df.iloc[0:0]  # ç©ºçš„ DataFrame
          else:
            assemble_entries = assemble_df[assemble_df['è¨‚å–®'] == order_num]

          print(f"assemble_entries ä¸­çš„è³‡æ–™ç­†æ•¸: {len(assemble_entries)}")

          # åªæ‹¿ã€Œè¨‚å–® + ç‰©æ–™ã€éƒ½è·Ÿé€™å€‹ material ç›¸åŒçš„å·¥åº
          material_num = material.material_num
          group_df = assemble_entries[assemble_entries['ç‰©æ–™'] == material_num].reset_index(drop=True)
          n = len(group_df)

          # é è¨­å…¨éƒ¨ isSimultaneously = False
          simultaneously_flags = [False] * n

          # å…ˆæƒä¸€æ¬¡ï¼šæ‰¾å‡ºã€Œä½œæ¥­æœ‰å€¼ä½†æ¨™æº–å…§æ–‡ç¢¼ç‚ºç©ºã€çš„åˆ—ï¼Œæ¨™è¨˜å®ƒçš„å‰ä¸€ç­† / ä¸‹ä¸€ç­†
          for i in range(n):
            seq_val = clean_nan(group_df.loc[i, 'ä½œæ¥­'])
            seq_str = str(seq_val).strip() if seq_val is not None else ''

            code_val = clean_nan(group_df.loc[i, 'æ¨™æº–å…§æ–‡ç¢¼'])
            code_str = str(code_val).strip() if code_val is not None else ''

            # ç¬¦åˆã€Œæœ‰ä½œæ¥­ã€æ²’æ¨™æº–å…§æ–‡ç¢¼ã€çš„æ¢ä»¶
            if seq_str and not code_str:
              # ä¸Šä¸€ç­†
              if i - 1 >= 0:
                # åŒä¸€å€‹ç‰©æ–™ï¼Œæ‰ç®—åŒçµ„ï¼ˆå…¶å¯¦ group_df å·²ç¶“æ˜¯åŒç‰©æ–™äº†ï¼‰
                simultaneously_flags[i - 1] = True
              # ä¸‹ä¸€ç­†
              if i + 1 < n:
                simultaneously_flags[i + 1] = True

          processed_work_nums = set()

          # å†æƒä¸€æ¬¡ï¼šçœŸæ­£å»ºç«‹ P_Assemble
          for i in range(n):
            assemble_row = group_df.loc[i]

            workNum = clean_nan(assemble_row.get('æ¨™æº–å…§æ–‡ç¢¼'))
            # ç©º workNum çš„é‚£ç­†ï¼ˆä¾‹å¦‚ä¸­é–“é‚£ç­†æ²’æœ‰æ¨™æº–å…§æ–‡ç¢¼ï¼‰æœ¬ä¾†å°±ä¸å»º P_Assemble
            if not workNum:
              print("    âš ï¸ workNum ç‚ºç©ºï¼Œç•¥éé€™ç­†å·¥åºã€‚")
              continue

            if workNum in processed_work_nums:
              print(f"    âš ï¸ workNum {workNum} é‡è¤‡ï¼Œå·²è™•ç†éï¼Œç•¥éã€‚")
              continue
            processed_work_nums.add(workNum)

            seq_num = clean_nan(assemble_row.get('ä½œæ¥­'))

            # å– code & step_code
            #if len(workNum) > 1:
            #  code = workNum[1:]
            #else:
            #  code = workNum  # å®‰å…¨ä¸€é»ï¼Œé¿å… workNum åªæœ‰ 1 å€‹å­—çš„æ™‚å€™ [1:] è®Šæˆç©ºå­—ä¸²

            #step_code = code_to_assembleStep.get(code, 0)
            part_info = part_info_map.get(workNum)
            #print("part_info_map", part_info_map)
            #print("part_info_map, workNum", workNum)
            #print("part_info_map, part_info", part_info)
            step_code = part_info['process_step_code']

            #print(f"    â–¶ code: {repr(code)}, step_code: {step_code}")
            print(f"    â–¶ step_code: {step_code}")

            abnormal_field = False

            # è®€å–ä½œæ¥­çŸ­æ–‡ï¼Œåˆ¤æ–·æ˜¯å¦ä»¥ 'Z' é–‹é ­ â†’ isStockIn
            op_short = clean_nan(assemble_row.get('ä½œæ¥­çŸ­æ–‡')) or ''
            op_short_str = str(op_short).strip()
            is_stock_in = op_short_str.startswith('Z')

            # é€™ä¸€åˆ—çš„ isSimultaneously
            is_simultaneously = simultaneously_flags[i]

            #if bom_df_is_empty:
            #  must_receive_qty=clean_nan(row.get('æ•¸é‡')) or 0
            #else:
            #  must_receive_qty=0

            assemble = P_Assemble(
                material_id=material.id,
                material_num=clean_nan(assemble_row.get('ç‰©æ–™')),
                material_comment=clean_nan(assemble_row.get('ç‰©æ–™èªªæ˜')),
                seq_num=seq_num,
                work_num=workNum,
                process_step_code=step_code,
                input_abnormal_disable=abnormal_field,
                user_id='',
                isStockIn=is_stock_in,
                isSimultaneously=is_simultaneously,
                #must_receive_qty=must_receive_qty,
                must_receive_qty=clean_nan(assemble_row.get('ä½œæ¥­æ•¸é‡ (MEINH)')),
                isShowBomGif = material_isBom,
                show2_ok = '3' if temp_bom_empty else '0',
            )
            s.add(assemble)

          # end loop_assemble
          s.commit()

          # commit å¾Œå†æŸ¥ä¸€æ¬¡çœ‹é€™å€‹ material_id åˆ°åº•æœ‰å¹¾ç­†
          cnt = s.query(P_Assemble).filter_by(material_id = material.id).count()
          print(f"  [P_Assemble] material_id={material.id} åœ¨ P_Assemble ç›®å‰ç¸½å…±æœ‰ {cnt} ç­†")

    # ç§»å‹•è™•ç†å®Œæˆçš„æª”æ¡ˆ
    try:
      unique_filename = get_unique_filename(_target_dir, _file_name, "copy")
      unique_target_path = os.path.join(_target_dir, unique_filename)
      print("unique_target_path:", unique_target_path)
      shutil.move(_path, unique_target_path)
      print(f"æª”æ¡ˆ {_path} å·²æˆåŠŸç§»å‹•åˆ° {unique_target_path}")
    except Exception as e:
      print(f"ç§»å‹•æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    continue

  #end for loop_1
  s.close()

  return jsonify({
    'status': return_value,
    'message': return_message1,
  })


@excelTable.route("/readAllExcelFiles", methods=['GET'])
def read_all_excel_files():
  print("readAllExcelFiles....")

  global global_var

  return_value = False
  return_message1 = 'éŒ¯èª¤, æ²’æœ‰å·¥å–®æª”æ¡ˆ!'
  file_count_total = 0  #æª”æ¡ˆç¸½æ•¸

  code_to_assembleStep = {    #çµ„è£å€å·¥ä½œé †åº, 3:æœ€å„ªå…ˆ
    '109': 3,
    '106': 1, '110': 2,
  }

  _base_dir = current_app.config['baseDir']
  _target_dir = _base_dir.replace("_in", "_out")
  print("read excel files, ç›®éŒ„: ", _base_dir)
  print("move excel files to, ç›®éŒ„: ", _target_dir)

  files = [
     f for f in os.listdir(_base_dir)
     if os.path.isfile(os.path.join(_base_dir, f))
     and f.endswith('.xlsx')
  ]

  if not files:
    return jsonify({'status': False, 'message': return_message1})

  sheet_names_to_check = [
    current_app.config['excel_product_sheet'],
    current_app.config['excel_bom_sheet'],
    current_app.config['excel_work_time_sheet']
  ]

  s = Session()

  for _file_name in files:  #æª”æ¡ˆè®€å–, for loop_1
    file_count_total +=1
    file_name_base = re.sub(r'_copy_\d+', '', _file_name)   # å¾æª”åä¸­ç§»é™¤åƒ _copy_1ã€_copy_2 é€™æ¨£çš„å­—ä¸²ï¼Œå–å¾—åŸå§‹æª”æ¡ˆåç¨±

    # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
    already_processed = s.query(
      exists().where(ProcessedFile.file_name == file_name_base)
    ).scalar()

    if already_processed:
      return_message1 = f"æª”æ¡ˆ {_file_name} å·²è™•ç†éï¼Œ è«‹å†é‡æ•´ç€è¦½å™¨!"
      print(return_message1)
      _path = os.path.join(_base_dir, _file_name)
      file_count_total -=1
    else:
      _path = _base_dir + '\\' + _file_name
      global_var = _path + ' æª”æ¡ˆè®€å–ä¸­...'

      with open(_path, 'rb') as file:   # with loop_1_a
        workbook = openpyxl.load_workbook(filename=file, read_only=True)
        return_value = True
        return_message1 = ''

        missing_sheets = [
          sheet for sheet in sheet_names_to_check if sheet not in workbook.sheetnames
        ]

        if missing_sheets:
          return jsonify({'status': False, 'message': 'éŒ¯èª¤, å·¥å–®æª”æ¡ˆå…§æ²’æœ‰ç›¸é—œå·¥ä½œè¡¨!'})

        print(sheet_names_to_check[0] + ' sheet exists, data reading...')

        material_df = pd.read_excel(_path, sheet_name=0)  # First sheet for Material
        material_df.rename(columns={c: norm_col(c) for c in material_df.columns}, inplace=True)
        material_df.columns = dedupe_columns(material_df.columns)

        # æ¬„ä½ç´šæ­£è¦åŒ–
        if 'å–®è™Ÿ' in material_df.columns:
          material_df['å–®è™Ÿ'] = (
              material_df['å–®è™Ÿ']
              .apply(normalize_order_number)
              .astype(str)
              .replace('nan', '')
          )

        bom_df = pd.read_excel(_path, sheet_name=1)

        """
        ###  å°±ç®—æ¬„ä½å« è¨‚å–® ï¼ˆå°¾å·´å¤šç©ºç™½ï¼‰ã€è¨‚å–®\nï¼ˆæ›è¡Œï¼‰ã€æˆ–æ”¹å« å–®è™Ÿï¼Œéƒ½èƒ½æ‰¾åˆ°,
        ###  çœŸçš„æ‰¾ä¸åˆ°å€™é¸æ¬„ä½æ™‚ï¼Œå°±å›é€€ç¬¬ä¸€æ¬„ï¼ˆExcel ç¬¬ä¸€æ¬„å°±æ˜¯è¨‚å–®/å–®è™Ÿï¼‰,
        ###  BOM éæ¿¾æ”¹ç”¨ bom_order_colï¼Œå°±ä¸æœƒå† KeyError: 'è¨‚å–®'
        ###
        bom_order_col = pick_order_col(bom_df)

        if bom_order_col is None:
            # æ²’æ¬„ä½å°±ç›´æ¥è¦–ç‚ºç©º BOM
            bom_df = pd.DataFrame(columns=["è¨‚å–®"])
            bom_order_col = "è¨‚å–®"
        else:
            # çµ±ä¸€è¨‚å–®æ¬„ä½å…§å®¹ï¼ˆnormalizeï¼‰
            bom_df[bom_order_col] = (
                bom_df[bom_order_col]
                  .apply(normalize_order_number)
                  .replace('nan', '')
                  .astype(str)
            )
            bom_df = bom_df.astype({bom_order_col: object})
        ###
        """

        # æ–‡æœ¬æ¬„ä½å¯è£œç©ºå­—ä¸²
        for col in ['ç‰©æ–™', 'ç‰©æ–™èªªæ˜']:
          if col in bom_df.columns:
            bom_df[col] = bom_df[col].fillna('')
        # æ•¸å€¼æ¬„ä½è½‰æ•¸å­—
        for col in ['ç‰©æ–™çŸ­ç¼º', 'éœ€æ±‚æ•¸é‡', 'é ç•™é …ç›®']:
          if col in bom_df.columns:
            bom_df[col] = pd.to_numeric(bom_df[col], errors='coerce').fillna(0).astype(int)

        assemble_df = pd.read_excel(_path, sheet_name=2).fillna('')

        # çµ±ä¸€è™•ç† BOM å’Œ Assemble çš„ è¨‚å–®æ¬„ä½
        if 'è¨‚å–®' in bom_df.columns:
          bom_df.iloc[:, 0] = (
              bom_df.iloc[:, 0]
              .apply(normalize_order_number)
              .replace('nan', '')
              .astype(str)   # ç›´æ¥æœ€å¾Œè½‰æˆ str â†’ object
          )

          # æ˜ç¢ºæŠŠ dtype è¨­æˆ object
          bom_df = bom_df.astype({bom_df.columns[0]: object})

        assemble_df.iloc[:, 0] = (
            assemble_df.iloc[:, 0]
            .apply(normalize_order_number)
            .replace('nan', '')
            .astype(str)
        )
        assemble_df = assemble_df.astype({assemble_df.columns[0]: object})

        # æ‰¾å‡ºå·¥æ™‚æ¬„ä½ï¼ˆå« .1 ç‰ˆæœ¬ï¼‰
        time_cols = [
          "B109çµ„è£å·¥æ™‚(åˆ†)", "B109çµ„è£å·¥æ™‚(åˆ†).1",
          "B106é›·åˆ»å·¥æ™‚(åˆ†)", "B106é›·åˆ»å·¥æ™‚(åˆ†).1",
          "B110æª¢é©—å·¥æ™‚(åˆ†)", "B110æª¢é©—å·¥æ™‚(åˆ†).1",
        ]
        time_cols = [c for c in time_cols if c in material_df.columns]

        # å·¥æ™‚æ¬„ä½è½‰æ•¸å­—ï¼ˆé¿å…å­—ä¸²/ç©ºç™½ï¼‰
        for c in time_cols:
          material_df[c] = pd.to_numeric(material_df[c], errors="coerce").fillna(0)

        # å…ˆçµ±ä¸€å–®è™Ÿ
        material_df["å–®è™Ÿ"] = material_df["å–®è™Ÿ"].apply(normalize_order_number)

        # å·¥æ™‚æ¬„ä½ sumï¼›å…¶ä»–æ¬„ä½å– first
        agg = {c: "sum" for c in time_cols}

        for c in material_df.columns:
            if c not in agg:
                agg[c] = "first"

        material_df = (
           material_df
           .groupby("å–®è™Ÿ", as_index=False)
           .agg(agg)
        )

        # ï¼ˆé™¤éŒ¯ç”¨ï¼‰
        print("after groupby rows:", len(material_df))

        # è™•ç† Material
        for _, row in material_df.iterrows(): # for loop_material
            #order_num = normalize_order_number(row.get('å–®è™Ÿ'))
            order_num = row.get('å–®è™Ÿ')
            if not order_num:
              print(f"[è­¦å‘Š] å–®è™Ÿç‚ºç©ºï¼Œè·³éè©²ç­†è³‡æ–™: {row.to_dict()}")
              continue

            tempQty = clean_nan(row.get('æ•¸é‡')) or 0
            temp_sd_time_B109 = float(clean_nan(row.get('B109çµ„è£å·¥æ™‚(åˆ†)')) or 0)
            temp_sd_time_B106 = float(clean_nan(row.get('B106é›·åˆ»å·¥æ™‚(åˆ†)')) or 0)
            temp_sd_time_B110 = float(clean_nan(row.get('B110æª¢é©—å·¥æ™‚(åˆ†)')) or 0)

            material = Material(
              order_num=order_num,
              material_num=clean_nan(row.get('æ–™è™Ÿ')) or '',
              material_comment=clean_nan(row.get('èªªæ˜')) or '',
              material_qty=tempQty,
              material_date=convert_date(row.get('ç«‹å–®æ—¥')),
              material_delivery_date=convert_date(row.get('äº¤æœŸ')),
              total_delivery_qty=tempQty,
              sd_time_B109="{:.2f}".format(float(temp_sd_time_B109)),
              sd_time_B106="{:.2f}".format(float(temp_sd_time_B106)),
              sd_time_B110="{:.2f}".format(float(temp_sd_time_B110)),
            )
            s.add(material)
            s.flush()  # ç¢ºä¿ material.id å¯ç”¨

            # Product
            product = Product(material_id=material.id)
            s.add(product)
            s.commit()

            required_col = 'è¨‚å–®'
            if required_col not in bom_df.columns:
              print('[ERROR] BOM æ¬„ä½ä¸å­˜åœ¨: è¨‚å–®')
              print('[DEBUG] BOM columns =', list(bom_df.columns))

              return jsonify({
                  "status": False,
                  "message": f"Excelæª”æ¡ˆå…§, æ‰¾ä¸åˆ°æ¬„ä½ã€Œ{required_col}ã€",
                  "columns": list(bom_df.columns),  # å‰ç«¯/é™¤éŒ¯å¯çœ‹
              })

            ###
            # BOM
            bom_entries = bom_df[bom_df['è¨‚å–®'] == order_num]
            #bom_entries = bom_df[bom_df[bom_order_col] == order_num]
            ###
            print(f"bom_entries ä¸­çš„è³‡æ–™ç­†æ•¸: {len(bom_entries)}")

            for _, bom_row in bom_entries.iterrows(): # for loop_bom
                shortage = to_int0(bom_row.get('ç‰©æ–™çŸ­ç¼º'))
                bom = Bom(
                  material_id=material.id,
                  seq_num=clean_nan(bom_row.get('é ç•™é …ç›®')),
                  material_num=clean_nan(bom_row.get('ç‰©æ–™')),
                  material_comment=clean_nan(bom_row.get('ç‰©æ–™èªªæ˜')),
                  req_qty=clean_nan(bom_row.get('éœ€æ±‚æ•¸é‡')),
                  start_date=convert_date(row.get('äº¤æœŸ')),
                  lack_bom_qty = shortage,
                  receive=(shortage == 0),
                )
                s.add(bom)
            s.commit()

            # Assemble
            assemble_entries = assemble_df[assemble_df.iloc[:, 0] == order_num]
            print(f"assemble_entries ä¸­çš„è³‡æ–™ç­†æ•¸: {len(assemble_entries)}")

            processed_work_nums = set()
            for _, assemble_row in assemble_entries.iterrows(): # for loop_assemble
              workNum = clean_nan(assemble_row.get('å·¥ä½œä¸­å¿ƒ'))
              if not workNum or workNum in processed_work_nums:
                  continue
              processed_work_nums.add(workNum)

              code = workNum[1:]
              step_code = code_to_assembleStep.get(code, 0)
              #abnormal_field = (workNum == 'B109')     # 2025-07-29 modify
              abnormal_field = False

              assemble = Assemble(
                material_id=material.id,
                material_num=clean_nan(assemble_row.get('ç‰©æ–™')),
                material_comment=clean_nan(assemble_row.get('ç‰©æ–™èªªæ˜')),
                seq_num=clean_nan(assemble_row.get('ä½œæ¥­')),
                work_num=workNum,
                process_step_code=step_code,
                input_abnormal_disable=abnormal_field,
                user_id=''
              )
              s.add(assemble)
            s.commit()
        # end for loop_material

        # âœ… è³‡æ–™è™•ç†å®Œå¾Œï¼Œè¨˜éŒ„æª”æ¡ˆè™•ç†ç´€éŒ„
        processed_file = ProcessedFile(file_name=file_name_base)
        s.add(processed_file)
        s.commit()

    # ç§»å‹•è™•ç†å®Œæˆçš„æª”æ¡ˆ
    try:
      unique_filename = get_unique_filename(_target_dir, _file_name, "copy")
      unique_target_path = os.path.join(_target_dir, unique_filename)
      print("unique_target_path:", unique_target_path)
      shutil.move(_path, unique_target_path)
      print(f"æª”æ¡ˆ {_path} å·²æˆåŠŸç§»å‹•åˆ° {unique_target_path}")
    except Exception as e:
      print(f"ç§»å‹•æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    continue
  #end for loop_1
  s.close()
  #end if condition_a

  return jsonify({
    'status': return_value,
    'message': return_message1,
  })


@excelTable.route("/deleteAssemblesWithNegativeGoodQty", methods=['GET'])
def delete_assembles_with_negative_good_qty():
  print("deleteAssemblesWithNegativeGoodQty...")

  s = Session()

  # æŸ¥è©¢æ‰€æœ‰ Assemble è³‡æ–™
  _objects = s.query(Assemble).all()

  # ç”¨ä¾†è¿½è¹¤å‰ä¸€ç­†è³‡æ–™
  previous_assemble = None

  # è¿­ä»£æ¯ä¸€ç­† Assemble è³‡æ–™
  for current_assemble in _objects:
    if current_assemble.good_qty < 0:
      # å¦‚æœç™¼ç¾ ask_qty å°æ–¼ 0ï¼Œå‰‡åˆªé™¤ç•¶å‰åŠå‰ä¸€ç­† Assemble ç´€éŒ„
      if previous_assemble:
        print(f"Deleting previous assemble: {previous_assemble.id}, good_qty={previous_assemble.good_qty}")
        s.delete(previous_assemble)  # åˆªé™¤å‰ä¸€ç­† Assemble ç´€éŒ„
      print(f"Deleting current assemble: {current_assemble.id}, ask_qty={current_assemble.good_qty}")
      s.delete(current_assemble)  # åˆªé™¤ç•¶å‰ Assemble ç´€éŒ„

      # è™•ç†èˆ‡ Material çš„é—œè¯ï¼ˆå¦‚æœ‰å¿…è¦ï¼‰
      # æ ¹æ“šæ¥­å‹™é‚è¼¯æ±ºå®šæ˜¯å¦åˆªé™¤ Materialï¼Œæˆ–æ›´æ–° Material çš„æŸäº›ç‹€æ…‹
      material = s.query(Material).filter(Material.id == current_assemble.material_id).first()
      if material:
        # æ›´æ–° Material çš„æ¬„ä½ï¼Œä¾‹å¦‚ isTakeOk æˆ–å…¶ä»–ç‹€æ…‹
        # material.isTakeOk = False  # æ›´æ–°æŸäº›ç‹€æ…‹
        # å¦‚æœéœ€è¦åˆªé™¤ Materialï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ï¼š
        # s.delete(material)
        print(f"Updating or deleting related material: {material.id}, material_num={material.material_num}")

    # æ›´æ–° previous_assemble ç‚ºç•¶å‰ Assemble
    previous_assemble = current_assemble

  # æäº¤åˆªé™¤
  s.commit()
  s.close()

  return jsonify({
    'status': True,
    #'message': return_message1,
  })


@excelTable.route("/deleteAssemblesWithNegativeGoodQtyP", methods=['GET'])
def delete_assembles_with_negative_good_qty_p():
  print("deleteAssemblesWithNegativeGoodQtyP...")

  s = Session()

  # æŸ¥è©¢æ‰€æœ‰ Assemble è³‡æ–™
  _objects = s.query(P_Assemble).all()

  # ç”¨ä¾†è¿½è¹¤å‰ä¸€ç­†è³‡æ–™
  previous_assemble = None

  # è¿­ä»£æ¯ä¸€ç­† Assemble è³‡æ–™
  for current_assemble in _objects:
    if current_assemble.good_qty < 0:
      # å¦‚æœç™¼ç¾ ask_qty å°æ–¼ 0ï¼Œå‰‡åˆªé™¤ç•¶å‰åŠå‰ä¸€ç­† Assemble ç´€éŒ„
      if previous_assemble:
        print(f"Deleting previous assemble: {previous_assemble.id}, good_qty={previous_assemble.good_qty}")
        s.delete(previous_assemble)  # åˆªé™¤å‰ä¸€ç­† Assemble ç´€éŒ„
      print(f"Deleting current assemble: {current_assemble.id}, ask_qty={current_assemble.good_qty}")
      s.delete(current_assemble)  # åˆªé™¤ç•¶å‰ Assemble ç´€éŒ„

      # è™•ç†èˆ‡ Material çš„é—œè¯ï¼ˆå¦‚æœ‰å¿…è¦ï¼‰
      # æ ¹æ“šæ¥­å‹™é‚è¼¯æ±ºå®šæ˜¯å¦åˆªé™¤ Materialï¼Œæˆ–æ›´æ–° Material çš„æŸäº›ç‹€æ…‹
      material = s.query(P_Material).filter(P_Material.id == current_assemble.material_id).first()
      if material:
        # æ›´æ–° Material çš„æ¬„ä½ï¼Œä¾‹å¦‚ isTakeOk æˆ–å…¶ä»–ç‹€æ…‹
        # material.isTakeOk = False  # æ›´æ–°æŸäº›ç‹€æ…‹
        # å¦‚æœéœ€è¦åˆªé™¤ Materialï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ï¼š
        # s.delete(material)
        print(f"Updating or deleting related material: {material.id}, material_num={material.material_num}")

    # æ›´æ–° previous_assemble ç‚ºç•¶å‰ Assemble
    previous_assemble = current_assemble

  # æäº¤åˆªé™¤
  s.commit()
  s.close()

  return jsonify({
    'status': True,
  })


@excelTable.route("/exportToExcelForError", methods=['POST'])
def export_to_excel_for_error():
    print("exportToExcelForError....")

    request_data = request.get_json()

    _blocks = request_data['blocks']
    _count = request_data['count']
    _name = request_data['name']

    print("_blocks:", _blocks)

    date = datetime.datetime.now()
    today = date.strftime('%Y-%m-%d-%H%M')
    file_name = 'çµ„è£ç•°å¸¸è¨˜éŒ„æŸ¥è©¢_'+today + '.xlsx'
    current_file = 'C:\\vue\\chumpower\\excel_export\\'+ file_name

    print("filename:", current_file)
    file_check = os.path.exists(current_file)  # true:excel file exist

    return_value = True  # true: export into excelæˆåŠŸ

    if file_check:
      try:
        os.remove(current_file)  # åˆªé™¤èˆŠæª”æ¡ˆ
      except PermissionError:
        print(f"ç„¡æ³•åˆªé™¤ {current_file}ï¼Œè«‹ç¢ºèªæ˜¯å¦å·²é—œé–‰è©²æª”æ¡ˆ")
        return_value = False
        return jsonify({"success": False, "message": "è«‹é—œé–‰ Excel æª”æ¡ˆå¾Œé‡è©¦"})
    #if file_check:
    #  wb = openpyxl.load_workbook(current_file)  # é–‹å•Ÿç¾æœ‰çš„ Excel æ´»é ç°¿ç‰©ä»¶
    #else:
    #  wb = Workbook()     # å»ºç«‹ç©ºç™½çš„ Excel æ´»é ç°¿ç‰©ä»¶
    wb = Workbook()     # å»ºç«‹ç©ºç™½çš„ Excel æ´»é ç°¿ç‰©ä»¶
    # ws = wb.active
    ws = wb.worksheets[0]   # å–å¾—ç¬¬ä¸€å€‹å·¥ä½œè¡¨

    ws.title = 'çµ„è£ç•°å¸¸è¨˜éŒ„æŸ¥è©¢-' + _name                # ä¿®æ”¹å·¥ä½œè¡¨ 1 çš„åç¨±ç‚º oxxo
    ws.sheet_properties.tabColor = '7da797'  # ä¿®æ”¹å·¥ä½œè¡¨ 1 é ç±¤é¡è‰²ç‚ºç´…è‰²

    for obj in _blocks:
      temp_array = []

      # åœ¨å¯«å…¥ Excel æ™‚åšè½‰æ›
      raw_str = obj['cause_message_str']  # e.g., "æ··æ–™(M01001),æ•£çˆª(M01002),æ‰çˆª(M01003)"
      messages = [re.sub(r'\(.*?\)', '', m).strip() for m in raw_str.split(',')]  # ç§»é™¤æ‹¬è™Ÿå’Œè£¡é¢å…§å®¹
      cleaned_str = ','.join(messages)  # "æ··æ–™,æ•£çˆª,æ‰çˆª"

      temp_array.append(obj['order_num'])
      temp_array.append(obj['comment'])
      temp_array.append(obj['delivery_date'])
      temp_array.append(obj['req_qty'])
      temp_array.append(obj['delivery_qty'])
      temp_array.append(obj['user'])
      #temp_array.append(obj['cause_message_str'])
      temp_array.append(cleaned_str)
      temp_array.append(obj['cause_user'])
      temp_array.append(obj['cause_date'])

      ws.append(temp_array)

    for col in ws.columns:
      column = col[0].column_letter  # Get the column name
      temp_cell = column + '1'
      ws[temp_cell].font = Font(
          name='å¾®è»Ÿæ­£é»‘é«”', color='ff0000', bold=True)  # è¨­å®šå„²å­˜æ ¼çš„æ–‡å­—æ¨£å¼
      ws[temp_cell].alignment = Alignment(horizontal='center')
      ws.column_dimensions[column].bestFit = True

    #wb.save(current_file)
    #é é˜² Excel æª”æ¡ˆè¢«é–å®š
    temp_file = current_file.replace(".xlsx", "_temp.xlsx")
    wb.save(temp_file)
    os.replace(temp_file, current_file)  # ç”¨æ–°æª”æ¡ˆå–ä»£èˆŠæª”æ¡ˆ

    print("file_name:", file_name)

    return jsonify({
      'status': return_value,
      'message': current_file,
      'file_name': file_name,
      # 'outputs': myDir,
    })


@excelTable.route("/exportToExcelForAssembleInformation", methods=['POST'])
def export_to_excel_for_assemble_information():
    print("exportToExcelForAssembleInformation....")

    request_data = request.get_json(force=True) or {}
    _blocks = request_data.get('blocks', [])
    _name = request_data.get('name', '')

    now = datetime.datetime.now()
    today = now.strftime('%Y-%m-%d-%H%M')
    file_name = f'çµ„è£å€åœ¨è£½å“ç”Ÿç”¢è³‡è¨ŠæŸ¥è©¢_{today}.xlsx'
    export_dir = r'C:\vue\chumpower\excel_export'
    os.makedirs(export_dir, exist_ok=True)
    current_file = os.path.join(export_dir, file_name)

    code_to_name = {
        1 : 'å‚™æ–™',
        19: 'ç­‰å¾…AGV(å‚™æ–™å€)',
        2 : 'AGVé‹è¡Œ(å‚™æ–™å€->çµ„è£å€)',
        20: 'AGVé‹è¡Œåˆ°çµ„è£å€',
        21: 'çµ„è£',
        22: 'æª¢é©—',
        23: 'é›·å°„',
        29: 'ç­‰å¾…AGV(çµ„è£å€)',
        3 : 'AGVé‹è¡Œ(çµ„è£å€->æˆå“å€)',
        30: 'AGVé‹è¡Œåˆ°æˆå“å€',
        31: 'æˆå“å…¥åº«',
        5 : 'å †é«˜æ©Ÿé‹è¡Œ(å‚™æ–™å€->çµ„è£å€)',
        6 : 'å †é«˜æ©Ÿé‹è¡Œ(çµ„è£å€->æˆå“å€)',
    }

    # process_type -> Material æ¨™å·¥æ¬„ä½
    ptype_to_sd_field = {
        21: "sd_time_B109",  # çµ„è£
        22: "sd_time_B110",  # æª¢é©—
        23: "sd_time_B106",  # é›·å°„(é›·åˆ»)
    }

    # AGV åç¨±è¦å‰‡ï¼ˆå«æ‹¬è™Ÿ/ä¸å«æ‹¬è™Ÿéƒ½è¦–ç‚ºè¦æ¸…ç©ºå§“åï¼‰
    AGV_NAMES = {"AGV1-1", "AGV1-2", "AGV2-1", "AGV2-2",
                 "(AGV1-1)", "(AGV1-2)", "(AGV2-1)", "(AGV2-2)"}

    def to_dt(val):
        if not val:
            return None
        if isinstance(val, datetime.datetime):
            return val
        if isinstance(val, datetime.date):
            return datetime.datetime.combine(val, datetime.time.min)
        if isinstance(val, str):
            for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S",
                        "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M"):
                try:
                    return datetime.datetime.strptime(val, fmt)
                except ValueError:
                    continue
        return None

    def fmt(dt):
        return dt.strftime("%Y-%m-%d %H:%M:%S") if isinstance(dt, datetime.datetime) else ""

    def _to_float(v, default=0.0):
        try:
            if v is None:
                return default
            s = str(v).strip()
            if s == "" or s.lower() == "nan":
                return default
            return float(s)
        except Exception:
            return default

    def _seconds_from_elapsed(val):
        """
        å°‡ elapsed / pause å¯èƒ½çš„å€¼è½‰æˆç§’æ•¸ï¼š
        - æ•¸å€¼å¯èƒ½æ˜¯ç§’æˆ–æ¯«ç§’ï¼ˆéå¤§è¦–ç‚ºæ¯«ç§’ï¼‰
        - å­—ä¸²å¯èƒ½æ˜¯ HH:MM:SS æˆ– MM:SS
        """
        if val is None:
            return None

        if isinstance(val, (int, float)):
            x = float(val)
            return x / 1000.0 if x > 1e7 else x

        if isinstance(val, str):
            s = val.strip()
            if not s:
                return None
            if ":" in s:
                parts = s.split(":")
                try:
                    parts = [float(p) for p in parts]
                    if len(parts) == 3:
                        h, m, sec = parts
                        return h * 3600 + m * 60 + sec
                    if len(parts) == 2:
                        m, sec = parts
                        return m * 60 + sec
                except Exception:
                    return None
            try:
                x = float(s)
                return x / 1000.0 if x > 1e7 else x
            except Exception:
                return None

        return None

    def get_active_seconds(process, start_dt, end_dt, effective_end):
        """
        å¯¦éš›æœ‰æ•ˆç§’æ•¸ï¼ˆæ’é™¤æš«åœï¼‰ï¼š
        1) å„ªå…ˆç”¨ elapsedActive_time / elapsed_active_time
        2) æ¬¡ç”¨ (end-start) - pause_time
        3) éƒ½æ²’æœ‰å°± (end-start)
        """
        for k in ("elapsedActive_time", "elapsed_active_time"):
            v = getattr(process, k, None)
            sec = _seconds_from_elapsed(v)
            if sec is not None and sec >= 0:
                return sec

        if start_dt and effective_end:
            total_sec = (effective_end - start_dt).total_seconds()
            if total_sec < 0:
                total_sec = 0

            pause_sec = None
            for k in ("pause_time", "paused_time", "pause_seconds", "paused_seconds"):
                v = getattr(process, k, None)
                pause_sec = _seconds_from_elapsed(v)
                if pause_sec is not None and pause_sec >= 0:
                    break

            if pause_sec:
                total_sec = max(total_sec - pause_sec, 0)

            return total_sec

        return None

    if os.path.exists(current_file):
        try:
            os.remove(current_file)
        except PermissionError:
            return jsonify({"status": False, "message": "è«‹é—œé–‰ Excel æª”æ¡ˆå¾Œé‡è©¦", "file_name": ""}), 200

    wb = Workbook()
    ws = wb.worksheets[0]
    ws.title = 'çµ„è£å€åœ¨è£½å“ç”Ÿç”¢è³‡è¨ŠæŸ¥è©¢-' + _name
    ws.sheet_properties.tabColor = '7da797'

    # âœ… åŠ å…¥ã€Œå“¡å·¥ã€æ¬„ä½ï¼ˆå·¥åº/å“¡å·¥åˆ†é–‹ï¼‰
    header = [
        'è¨‚å–®ç·¨è™Ÿ','ç‰©æ–™', 'èªªæ˜', 'äº¤æœŸ', 'è¨‚å–®æ•¸é‡', 'ç¾æ³æ•¸é‡',
        'å·¥åº', 'å“¡å·¥',
        'é–‹å§‹æ™‚é–“', 'çµæŸæ™‚é–“',
        'å¯¦éš›è€—æ™‚(åˆ†)', 'å¯¦éš›å·¥æ™‚(åˆ†/PCS)', 'å–®ä»¶æ¨™å·¥(åˆ†/PCS)', 'è¨»è¨˜'
    ]
    ws.append(header)

    for idx, _ in enumerate(header, start=1):
        cell = ws.cell(row=1, column=idx)
        cell.font = Font(name='å¾®è»Ÿæ­£é»‘é«”', color='FF0000', bold=True)
        cell.alignment = Alignment(horizontal='center')
        ws.column_dimensions[cell.column_letter].width = 16

    s = Session()
    temp_file = current_file.replace(".xlsx", "_temp.xlsx")

    try:
        ###
        ORDER_COL = header.index('è¨‚å–®ç·¨è™Ÿ') + 1  # ç›®å‰å°±æ˜¯ 1
        prev_order_num = None
        group_idx = -1

        fill_white = PatternFill("solid", fgColor="FFFFFFFF")
        fill_gray  = PatternFill("solid", fgColor="FFEDF2F4")  # #edf2f4
        ###
        for obj in _blocks:
            order_num = obj.get('order_num', '')
            if not order_num:
                continue

            ###
            # âœ… è¨‚å–®ç¾¤çµ„åˆ‡æ›ï¼šåŒä¸€å¼µè¨‚å–®çš„å¤šåˆ—è¦åŒè‰²ï¼Œä¸åŒè¨‚å–®é–“äº¤éŒ¯
            if order_num != prev_order_num:
                group_idx += 1
                prev_order_num = order_num

            group_fill = fill_white if (group_idx % 2 == 0) else fill_gray
            ###

            material = s.query(Material).filter(Material.order_num == order_num).first()

            work_qty = 0
            if material and getattr(material, "total_delivery_qty", None):
                work_qty = _to_float(material.total_delivery_qty, 0)
            else:
                work_qty = _to_float(obj.get("req_qty", 0), 0)

            processes = material._process if (material and getattr(material, "_process", None)) else []
            assemble_records = material._assemble if (material and getattr(material, "_assemble", None)) else []

            if not processes:
                ws.append([
                    order_num,
                    obj.get('material_num', ''),
                    obj.get('comment', ''),
                    obj.get('delivery_date', ''),
                    obj.get('req_qty', ''),
                    obj.get('delivery_qty', ''),
                    '', '',  # å·¥åº / å“¡å·¥
                    '', '',  # é–‹å§‹ / çµæŸ
                    '', '', '',  # å¯¦éš›è€—æ™‚ / å¯¦éš›å·¥æ™‚ / å–®ä»¶æ¨™å·¥
                    ''
                ])

                ###
                r = ws.max_row
                c = ws.cell(row=r, column=ORDER_COL)
                c.number_format = '@'
                c.value = str(order_num)
                c.fill = group_fill
                ###

                continue

            for process in processes:
                ###
                alarm_proc_record = [a for a in assemble_records if ((a.id == process.assemble_id and process.has_started))]
                if len(alarm_proc_record) == 1:
                    alarm_msg_enable = alarm_proc_record[0].alarm_enable
                    alarm_msg_isAssembleFirstAlarm = alarm_proc_record[0].isAssembleFirstAlarm
                    if not alarm_msg_enable and not alarm_msg_isAssembleFirstAlarm:
                      alarm_msg_string = (alarm_proc_record[0].alarm_message or '').strip()
                    else:
                      alarm_msg_string = ''

                    if process.process_type == 21:
                      alarm_msg_string = alarm_proc_record[0].Incoming1_Abnormal
                else:
                    alarm_msg_enable = True
                    alarm_msg_isAssembleFirstAlarm = True
                    alarm_msg_string = ''

                    if (
                      material.Incoming0_Abnormal != '' and
                      process.end_time !='' and
                      process.begin_time !='' and
                      process.assemble_id==0 and
                      process.process_type in [1, 5]
                    ):
                      alarm_msg_string = material.Incoming0_Abnormal
                ###

                ptype = getattr(process, "process_type", None)

                # âœ… å·¥åºæ¬„ï¼šåªæ”¾å·¥åºåç¨±
                #process_name = code_to_name.get(ptype, 'ç©ºç™½')
                process_name_base = code_to_name.get(ptype, 'ç©ºç™½')

                ## âœ… è‹¥è©²å·¥åºç‚ºç•°å¸¸ï¼šprocess.normal_work_time = 0 æˆ– 2 â†’ å·¥åºåç¨±åŠ ä¸Š '- ç•°å¸¸æ•´ä¿®'
                #nwt = getattr(process, "normal_work_time", None)
                #try:
                #    nwt_i = int(float(nwt)) if nwt is not None and str(nwt).strip() != "" else None
                #except Exception:
                #    nwt_i = None

                #process_name = f"{process_name} - ç•°å¸¸æ•´ä¿®" if (not alarm_msg_enable and not alarm_msg_isAssembleFirstAlarm) else f"{process_name}"
                process_name_display = process_name_base + (" - ç•°å¸¸æ•´ä¿®" if (not alarm_msg_enable and not alarm_msg_isAssembleFirstAlarm) else "")

                # âœ… å“¡å·¥æ¬„ï¼š({å“¡å·¥ç·¨è™Ÿ}{å“¡å·¥å§“å})ï¼ŒAGV è¦å‰‡å§“åæ¸…ç©º
                emp_col = ""
                emp_id = (getattr(process, "user_id", "") or "").strip()

                # âœ… è‹¥ã€Œå“¡å·¥ç·¨è™Ÿã€æœ¬èº«å°±æ˜¯ AGV1-1/AGV1-2/AGV2-1/AGV2-2ï¼ˆå«æ‹¬è™Ÿä¹Ÿæ”¯æ´ï¼‰â†’ æ•´æ ¼å“¡å·¥æ¬„ç©ºç™½
                AGV_IDS = {"AGV1-1", "AGV1-2", "AGV2-1", "AGV2-2", "(AGV1-1)", "(AGV1-2)", "(AGV2-1)", "(AGV2-2)"}
                if emp_id in AGV_IDS:
                    emp_col = ""
                elif emp_id:
                    user = s.query(User).filter_by(emp_id=emp_id).first()
                    emp_short = emp_id.lstrip("0") if isinstance(emp_id, str) else str(emp_id)

                    emp_name = (getattr(user, "emp_name", "") or "").strip() if user else ""

                    # é€™æ¢ä½ åŸæœ¬çš„è¦å‰‡ä»ä¿ç•™ï¼šè‹¥å§“åæ˜¯ AGVâ€¦ â†’ å§“åæ¸…ç©ºï¼ˆä½†ä¸æœƒå½±éŸ¿ AGV ç·¨è™Ÿï¼Œå› ç‚ºä¸Šé¢å·²æ””æˆªï¼‰
                    AGV_NAMES = {"AGV1-1", "AGV1-2", "AGV2-1", "AGV2-2", "(AGV1-1)", "(AGV1-2)", "(AGV2-1)", "(AGV2-2)"}
                    if emp_name in AGV_NAMES:
                        emp_name = ""

                    emp_col = f"({emp_short}{emp_name})"

                start_dt = to_dt(getattr(process, "begin_time", None))
                end_dt = to_dt(getattr(process, "end_time", None)) if ptype != 31 else None
                effective_end = end_dt if end_dt is not None else (now if start_dt else None)

                active_sec = get_active_seconds(process, start_dt, end_dt, effective_end)
                actual_minutes = ""
                actual_per_pcs = ""
                if active_sec is not None:
                    actual_minutes = round(active_sec / 60.0, 2)
                    if work_qty and work_qty > 0:
                        actual_per_pcs = round(actual_minutes / work_qty, 4)

                std_per_pcs = ""
                sd_field = ptype_to_sd_field.get(ptype)
                if sd_field and material and hasattr(material, sd_field):
                    std_total_min = _to_float(getattr(material, sd_field, 0), 0)
                    if work_qty and work_qty > 0 and std_total_min:
                        std_per_pcs = round(std_total_min / work_qty, 4)

                ###
                ws.append([
                    order_num,
                    obj.get('material_num', ''),
                    obj.get('comment', ''),
                    obj.get('delivery_date', ''),
                    obj.get('req_qty', ''),
                    obj.get('delivery_qty', ''),
                    #process_name,   # âœ… å·¥åº
                    process_name_display,
                    emp_col,        # âœ… å“¡å·¥
                    fmt(start_dt),
                    fmt(end_dt),
                    actual_minutes,
                    actual_per_pcs,
                    std_per_pcs,
                    alarm_msg_string
                ])

                ###
                r = ws.max_row
                c = ws.cell(row=r, column=ORDER_COL)
                c.number_format = '@'
                c.value = str(order_num)
                c.fill = group_fill
                ###

                ###
                if not alarm_msg_enable and not alarm_msg_isAssembleFirstAlarm:
                  r = ws.max_row  # å‰›å¯«å…¥çš„é‚£ä¸€åˆ—
                  PROCESS_COL = header.index('å·¥åº') + 1  # header ä½ å·²ç¶“æœ‰äº† :contentReference[oaicite:4]{index=4}
                  cell = ws.cell(row=r, column=PROCESS_COL)

                  rt = CellRichText()
                  rt.append(TextBlock(InlineFont(rFont='å¾®è»Ÿæ­£é»‘é«”', sz=11), process_name_base))
                  rt.append(TextBlock(InlineFont(rFont='å¾®è»Ÿæ­£é»‘é«”', sz=11, color='FFFF0000'), " - ç•°å¸¸æ•´ä¿®"))
                  cell.value = rt
                ###

        ###
        ORDER_COL   = header.index('è¨‚å–®ç·¨è™Ÿ') + 1
        PROCESS_COL = header.index('å·¥åº') + 1
        EMP_COL     = header.index('å“¡å·¥') + 1

        oL = get_column_letter(ORDER_COL)
        eL = get_column_letter(EMP_COL)

        # âœ… è¨‚å–®ç·¨è™Ÿã€å·¥åºã€å“¡å·¥ çš„æ¨™é ­éƒ½æœƒæœ‰ä¸‹æ‹‰ç¯©é¸ï¼ˆç¯„åœå«ä¸­é–“æ‰€æœ‰æ¬„ï¼‰
        ws.auto_filter.ref = f"{oL}1:{eL}{ws.max_row}"
        ###

        wb.save(temp_file)
        os.replace(temp_file, current_file)

        return jsonify({'status': True, 'message': current_file, 'file_name': file_name}), 200

    except Exception as e:
        s.rollback()
        try:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        except Exception:
            pass
        print("export_to_excel_for_assemble_information EXCEPTION:", repr(e))
        return jsonify({'status': False, 'message': f'åŒ¯å‡ºå¤±æ•—: {e}', 'file_name': ''}), 500

    finally:
        s.close()


@excelTable.route("/exportToExcelForProcessInformation", methods=['POST'])
def export_to_excel_for_process_information():
    print("exportToExcelForProcessInformation....")

    request_data = request.get_json(force=True) or {}
    _blocks = request_data.get('blocks', [])
    _name = request_data.get('name', '')

    now = datetime.datetime.now()
    today = now.strftime('%Y-%m-%d-%H%M')
    file_name = f'åŠ å·¥å€åœ¨è£½å“ç”Ÿç”¢è³‡è¨ŠæŸ¥è©¢_{today}.xlsx'
    export_dir = r'C:\vue\chumpower\excel_export'
    os.makedirs(export_dir, exist_ok=True)
    current_file = os.path.join(export_dir, file_name)

    code_to_name = {
        1 : 'é ˜æ–™',
        #19: 'ç­‰å¾…AGV(å‚™æ–™å€)',
        #2 : 'AGVé‹è¡Œ(å‚™æ–™å€->çµ„è£å€)',
        #20: 'AGVé‹è¡Œåˆ°çµ„è£å€',
        #21: 'çµ„è£',
        #22: 'æª¢é©—',
        #23: 'é›·å°„',
        #29: 'ç­‰å¾…AGV(çµ„è£å€)',
        #3 : 'AGVé‹è¡Œ(çµ„è£å€->æˆå“å€)',
        #30: 'AGVé‹è¡Œåˆ°æˆå“å€',
        31: 'æˆå“å…¥åº«',
        5 : 'å †é«˜æ©Ÿé‹è¡Œ(é ˜æ–™å€->åŠ å·¥å€)',
        6 : 'å †é«˜æ©Ÿé‹è¡Œ(åŠ å·¥å€->æˆå“å€)',
    }

    # process_type -> Material æ¨™å·¥æ¬„ä½
    ptype_to_sd_field = {
        21: "sd_time_B109",  # çµ„è£
        22: "sd_time_B110",  # æª¢é©—
        23: "sd_time_B106",  # é›·å°„(é›·åˆ»)
    }

    ### for process add block ###
    part_info_map = {}
    step_to_part_code_map = {}
    for p in s.query(P_Part).all():
      code = (p.part_code or '').strip()
      if not code:
        continue
      step = int(p.process_step_code or 0)

      part_info_map[code] = {
        'comment': (p.part_comment or '').strip(),
        'process_step_code': step
      }

      # åæŸ¥ï¼šstep_code -> part_code
      # è‹¥åŒ step_code æœ‰å¤šç­†ï¼Œä½ å¯ä»¥æ±ºå®šè¦ä¸è¦è¦†è“‹
      if step and step not in step_to_part_code_map:
        step_to_part_code_map[step] = code
    ### end block ###

    # AGV åç¨±è¦å‰‡ï¼ˆå«æ‹¬è™Ÿ/ä¸å«æ‹¬è™Ÿéƒ½è¦–ç‚ºè¦æ¸…ç©ºå§“åï¼‰
    AGV_NAMES = {"AGV1-1", "AGV1-2", "AGV2-1", "AGV2-2",
                 "(AGV1-1)", "(AGV1-2)", "(AGV2-1)", "(AGV2-2)"}

    def to_dt(val):
        if not val:
            return None
        if isinstance(val, datetime.datetime):
            return val
        if isinstance(val, datetime.date):
            return datetime.datetime.combine(val, datetime.time.min)
        if isinstance(val, str):
            for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S",
                        "%Y-%m-%d %H:%M", "%Y/%m/%d %H:%M"):
                try:
                    return datetime.datetime.strptime(val, fmt)
                except ValueError:
                    continue
        return None

    def fmt(dt):
        return dt.strftime("%Y-%m-%d %H:%M:%S") if isinstance(dt, datetime.datetime) else ""

    def _to_float(v, default=0.0):
      try:
        if v is None:
          return default
        s = str(v).strip()
        if s == "" or s.lower() == "nan":
          return default
        return float(s)
      except Exception:
        return default

    def _seconds_from_elapsed(val):
        """
        å°‡ elapsed / pause å¯èƒ½çš„å€¼è½‰æˆç§’æ•¸ï¼š
        - æ•¸å€¼å¯èƒ½æ˜¯ç§’æˆ–æ¯«ç§’ï¼ˆéå¤§è¦–ç‚ºæ¯«ç§’ï¼‰
        - å­—ä¸²å¯èƒ½æ˜¯ HH:MM:SS æˆ– MM:SS
        """
        if val is None:
          return None

        if isinstance(val, (int, float)):
          x = float(val)
          return x / 1000.0 if x > 1e7 else x

        if isinstance(val, str):
          s = val.strip()
          if not s:
            return None
          if ":" in s:
            parts = s.split(":")
            try:
              parts = [float(p) for p in parts]
              if len(parts) == 3:
                h, m, sec = parts
                return h * 3600 + m * 60 + sec
              if len(parts) == 2:
                m, sec = parts
                return m * 60 + sec
            except Exception:
              return None
          try:
            x = float(s)
            return x / 1000.0 if x > 1e7 else x
          except Exception:
            return None

        #end if_loop
        return None

    def get_active_seconds(process, start_dt, end_dt, effective_end):
        """
        å¯¦éš›æœ‰æ•ˆç§’æ•¸ï¼ˆæ’é™¤æš«åœï¼‰ï¼š
        1) å„ªå…ˆç”¨ elapsedActive_time / elapsed_active_time
        2) æ¬¡ç”¨ (end-start) - pause_time
        3) éƒ½æ²’æœ‰å°± (end-start)
        """
        for k in ("elapsedActive_time", "elapsed_active_time"):
            v = getattr(process, k, None)
            sec = _seconds_from_elapsed(v)
            if sec is not None and sec >= 0:
                return sec

        if start_dt and effective_end:
            total_sec = (effective_end - start_dt).total_seconds()
            if total_sec < 0:
                total_sec = 0

            pause_sec = None
            for k in ("pause_time", "paused_time", "pause_seconds", "paused_seconds"):
                v = getattr(process, k, None)
                pause_sec = _seconds_from_elapsed(v)
                if pause_sec is not None and pause_sec >= 0:
                    break

            if pause_sec:
                total_sec = max(total_sec - pause_sec, 0)

            return total_sec

        return None

    if os.path.exists(current_file):
      try:
        os.remove(current_file)
      except PermissionError:
        return jsonify({"status": False, "message": "è«‹é—œé–‰ Excel æª”æ¡ˆå¾Œé‡è©¦", "file_name": ""}), 200

    wb = Workbook()
    ws = wb.worksheets[0]
    ws.title = 'åŠ å·¥å€åœ¨è£½å“ç”Ÿç”¢è³‡è¨ŠæŸ¥è©¢-' + _name
    ws.sheet_properties.tabColor = '7da797'

    # âœ… åŠ å…¥ã€Œå“¡å·¥ã€æ¬„ä½ï¼ˆå·¥åº/å“¡å·¥åˆ†é–‹ï¼‰
    header = [
        'è¨‚å–®ç·¨è™Ÿ','ç‰©æ–™', 'èªªæ˜', 'äº¤æœŸ', 'è¨‚å–®æ•¸é‡', 'ç¾æ³æ•¸é‡',
        'å·¥åº', 'å“¡å·¥',
        'é–‹å§‹æ™‚é–“', 'çµæŸæ™‚é–“',
        'å¯¦éš›è€—æ™‚(åˆ†)', 'å¯¦éš›å·¥æ™‚(åˆ†/PCS)', 'å–®ä»¶æ¨™å·¥(åˆ†/PCS)', 'è¨»è¨˜'
    ]
    ws.append(header)

    for idx, _ in enumerate(header, start=1):
        cell = ws.cell(row=1, column=idx)
        cell.font = Font(name='å¾®è»Ÿæ­£é»‘é«”', color='FF0000', bold=True)
        cell.alignment = Alignment(horizontal='center')
        ws.column_dimensions[cell.column_letter].width = 16

    s = Session()
    temp_file = current_file.replace(".xlsx", "_temp.xlsx")

    try:
        ###
        ORDER_COL = header.index('è¨‚å–®ç·¨è™Ÿ') + 1  # ç›®å‰å°±æ˜¯ 1
        prev_order_num = None
        group_idx = -1

        fill_white = PatternFill("solid", fgColor="FFFFFFFF")
        fill_gray  = PatternFill("solid", fgColor="FFEDF2F4")  # #edf2f4
        ###
        for obj in _blocks:
            order_num = obj.get('order_num', '')
            if not order_num:
              continue

            ###
            # âœ… è¨‚å–®ç¾¤çµ„åˆ‡æ›ï¼šåŒä¸€å¼µè¨‚å–®çš„å¤šåˆ—è¦åŒè‰²ï¼Œä¸åŒè¨‚å–®é–“äº¤éŒ¯
            if order_num != prev_order_num:
              group_idx += 1
              prev_order_num = order_num

            group_fill = fill_white if (group_idx % 2 == 0) else fill_gray
            ###

            material = s.query(P_Material).filter(P_Material.order_num == order_num).first()

            work_qty = 0
            if material and getattr(material, "total_delivery_qty", None):
              work_qty = _to_float(material.total_delivery_qty, 0)
            else:
              work_qty = _to_float(obj.get("req_qty", 0), 0)

            processes = material._process if (material and getattr(material, "_process", None)) else []
            assemble_records = material._assemble if (material and getattr(material, "_assemble", None)) else []

            if not processes:
                ws.append([
                    order_num,
                    obj.get('material_num', ''),
                    obj.get('comment', ''),
                    obj.get('delivery_date', ''),
                    obj.get('req_qty', ''),
                    obj.get('delivery_qty', ''),
                    '', '',       # å·¥åº / å“¡å·¥
                    '', '',       # é–‹å§‹ / çµæŸ
                    '', '', '',   # å¯¦éš›è€—æ™‚ / å¯¦éš›å·¥æ™‚ / å–®ä»¶æ¨™å·¥
                    ''            # è¨»è¨˜
                ])

                ###
                r = ws.max_row
                c = ws.cell(row=r, column=ORDER_COL)
                c.number_format = '@'
                c.value = str(order_num)
                c.fill = group_fill
                ###

                continue

            for process in processes:
                ###
                alarm_proc_record = [a for a in assemble_records if ((a.id == process.assemble_id and process.has_started))]
                if len(alarm_proc_record) == 1:
                    alarm_msg_enable = alarm_proc_record[0].alarm_enable
                    alarm_msg_isAssembleFirstAlarm = alarm_proc_record[0].isAssembleFirstAlarm
                    if not alarm_msg_enable and not alarm_msg_isAssembleFirstAlarm:
                      alarm_msg_string = (alarm_proc_record[0].alarm_message or '').strip()
                    else:
                      alarm_msg_string = ''

                    if process.process_type == 21:
                      alarm_msg_string = alarm_proc_record[0].Incoming1_Abnormal
                else:
                    alarm_msg_enable = True
                    alarm_msg_isAssembleFirstAlarm = True
                    alarm_msg_string = ''

                    if (
                      material.Incoming0_Abnormal != '' and
                      process.end_time !='' and
                      process.begin_time !='' and
                      process.assemble_id==0 and
                      process.process_type in [1, 5]
                    ):
                      alarm_msg_string = material.Incoming0_Abnormal
                ###

                ptype = getattr(process, "process_type", None)

                # âœ… å·¥åºæ¬„ï¼šåªæ”¾å·¥åºåç¨±
                #process_name = code_to_name.get(ptype, 'ç©ºç™½')
                process_name_base = code_to_name.get(ptype, 'ç©ºç™½')

                process_name_display = process_name_base + (" - ç•°å¸¸æ•´ä¿®" if (not alarm_msg_enable and not alarm_msg_isAssembleFirstAlarm) else "")

                # âœ… å“¡å·¥æ¬„ï¼š({å“¡å·¥ç·¨è™Ÿ}{å“¡å·¥å§“å})ï¼ŒAGV è¦å‰‡å§“åæ¸…ç©º
                emp_col = ""
                emp_id = (getattr(process, "user_id", "") or "").strip()

                # âœ… è‹¥ã€Œå“¡å·¥ç·¨è™Ÿã€æœ¬èº«å°±æ˜¯ AGV1-1/AGV1-2/AGV2-1/AGV2-2ï¼ˆå«æ‹¬è™Ÿä¹Ÿæ”¯æ´ï¼‰â†’ æ•´æ ¼å“¡å·¥æ¬„ç©ºç™½
                AGV_IDS = {"AGV1-1", "AGV1-2", "AGV2-1", "AGV2-2", "(AGV1-1)", "(AGV1-2)", "(AGV2-1)", "(AGV2-2)"}
                if emp_id in AGV_IDS:
                    emp_col = ""
                elif emp_id:
                    user = s.query(User).filter_by(emp_id=emp_id).first()
                    emp_short = emp_id.lstrip("0") if isinstance(emp_id, str) else str(emp_id)

                    emp_name = (getattr(user, "emp_name", "") or "").strip() if user else ""

                    # é€™æ¢ä½ åŸæœ¬çš„è¦å‰‡ä»ä¿ç•™ï¼šè‹¥å§“åæ˜¯ AGVâ€¦ â†’ å§“åæ¸…ç©ºï¼ˆä½†ä¸æœƒå½±éŸ¿ AGV ç·¨è™Ÿï¼Œå› ç‚ºä¸Šé¢å·²æ””æˆªï¼‰
                    AGV_NAMES = {"AGV1-1", "AGV1-2", "AGV2-1", "AGV2-2", "(AGV1-1)", "(AGV1-2)", "(AGV2-1)", "(AGV2-2)"}
                    if emp_name in AGV_NAMES:
                        emp_name = ""

                    emp_col = f"({emp_short}{emp_name})"

                start_dt = to_dt(getattr(process, "begin_time", None))
                end_dt = to_dt(getattr(process, "end_time", None)) if ptype != 31 else None
                effective_end = end_dt if end_dt is not None else (now if start_dt else None)

                active_sec = get_active_seconds(process, start_dt, end_dt, effective_end)
                actual_minutes = ""
                actual_per_pcs = ""
                if active_sec is not None:
                    actual_minutes = round(active_sec / 60.0, 2)
                    if work_qty and work_qty > 0:
                        actual_per_pcs = round(actual_minutes / work_qty, 4)

                std_per_pcs = ""
                sd_field = ptype_to_sd_field.get(ptype)
                if sd_field and material and hasattr(material, sd_field):
                    std_total_min = _to_float(getattr(material, sd_field, 0), 0)
                    if work_qty and work_qty > 0 and std_total_min:
                        std_per_pcs = round(std_total_min / work_qty, 4)

                ###
                ws.append([
                    order_num,
                    obj.get('material_num', ''),
                    obj.get('comment', ''),
                    obj.get('delivery_date', ''),
                    obj.get('req_qty', ''),
                    obj.get('delivery_qty', ''),
                    process_name_display,
                    emp_col,        # âœ… å“¡å·¥
                    fmt(start_dt),
                    fmt(end_dt),
                    actual_minutes,
                    actual_per_pcs,
                    std_per_pcs,
                    alarm_msg_string
                ])

                ###
                r = ws.max_row
                c = ws.cell(row=r, column=ORDER_COL)
                c.number_format = '@'
                c.value = str(order_num)
                c.fill = group_fill
                ###

                ###
                if not alarm_msg_enable and not alarm_msg_isAssembleFirstAlarm:
                  r = ws.max_row  # å‰›å¯«å…¥çš„é‚£ä¸€åˆ—
                  PROCESS_COL = header.index('å·¥åº') + 1  # header ä½ å·²ç¶“æœ‰äº† :contentReference[oaicite:4]{index=4}
                  cell = ws.cell(row=r, column=PROCESS_COL)

                  rt = CellRichText()
                  rt.append(TextBlock(InlineFont(rFont='å¾®è»Ÿæ­£é»‘é«”', sz=11), process_name_base))
                  rt.append(TextBlock(InlineFont(rFont='å¾®è»Ÿæ­£é»‘é«”', sz=11, color='FFFF0000'), " - ç•°å¸¸æ•´ä¿®"))
                  cell.value = rt
                ###

        ###
        ORDER_COL   = header.index('è¨‚å–®ç·¨è™Ÿ') + 1
        PROCESS_COL = header.index('å·¥åº') + 1
        EMP_COL     = header.index('å“¡å·¥') + 1

        oL = get_column_letter(ORDER_COL)
        eL = get_column_letter(EMP_COL)

        # âœ… è¨‚å–®ç·¨è™Ÿã€å·¥åºã€å“¡å·¥ çš„æ¨™é ­éƒ½æœƒæœ‰ä¸‹æ‹‰ç¯©é¸ï¼ˆç¯„åœå«ä¸­é–“æ‰€æœ‰æ¬„ï¼‰
        ws.auto_filter.ref = f"{oL}1:{eL}{ws.max_row}"
        ###

        wb.save(temp_file)
        os.replace(temp_file, current_file)

        return jsonify({'status': True, 'message': current_file, 'file_name': file_name}), 200

    except Exception as e:
        s.rollback()
        try:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        except Exception:
            pass
        print("export_to_excel_for_process_information EXCEPTION:", repr(e))
        return jsonify({'status': False, 'message': f'åŒ¯å‡ºå¤±æ•—: {e}', 'file_name': ''}), 500

    finally:
        s.close()


# ä¸Šå‚³.xlsxå·¥å–®æª”æ¡ˆçš„ API
@excelTable.route('/uploadExcelFile', methods=['POST'])
def upload_excel_file():
  print("uploadExcelFile....")

  _base_dir = current_app.config['baseDir']

  # ä»¥ baseDir çš„ä¸Šå±¤ä½œç‚ºæ ¹ç›®éŒ„ï¼ˆé€šå¸¸ baseDir æœƒæ˜¯ .../excel_inï¼‰
  base_dir_default = os.path.abspath(current_app.config['baseDir'])
  root_dir = os.path.abspath(os.path.dirname(base_dir_default))

  if not os.path.exists(_base_dir):
    os.makedirs(_base_dir)

  file = request.files.get('file')

  _upload_type = request.form.get('uploadType')
  upload_type=_upload_type.strip()
  print("uploadType:", upload_type)

  if not file:
    return jsonify({'message': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'}), 400

  # ç¢ºä¿æ˜¯ Excel æª”æ¡ˆ
  if not file.filename.endswith(('.xlsx', '.xls')):
    return jsonify({'message': 'åªå…è¨±ä¸Šå‚³ Excel æª”æ¡ˆ'}), 400

  if file.filename == '':
    return jsonify({'message': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'}), 400

  if upload_type == "excelp":
    print("step1...")
    _base_dir = os.path.join(os.path.dirname(_base_dir), "excel_in_p")
    print("base_dir:", _base_dir)

  if upload_type == "excelm":
    print("step2...")
    _base_dir = os.path.join(os.path.dirname(_base_dir), "excel_modify")
    print("base_dir:", _base_dir)

  #

  #
  file_path = os.path.join(_base_dir, file.filename)
  print("file_path:",file_path)

  # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
  if os.path.exists(file_path):
    #return jsonify({'message': f'æª”æ¡ˆ "{file.filename}" å·²å­˜åœ¨'}), 400
    unique_filename = get_unique_filename(_base_dir, file.filename, "copy")
    file_path = os.path.join(_base_dir, unique_filename)

  file.save(file_path)

  try:
    with open(file_path, 'rb') as f:
        print(f"æª”æ¡ˆæˆåŠŸå„²å­˜ä¸¦å¯è®€å–: {file_path}")
        return jsonify({
          'message': 'ä¸Šå‚³æˆåŠŸ',
          'filename': file.filename,
          'status': True,
        })
  except Exception as e:
      print(f"æª”æ¡ˆå„²å­˜å¾Œè®€å–å¤±æ•—: {str(e)}")
      return jsonify({'message': f'æª”æ¡ˆå„²å­˜å¾Œè®€å–å¤±æ•—: {str(e)}'}), 500
  #return jsonify({'message': 'ä¸Šå‚³æˆåŠŸ', 'filename': file.filename}), 200


@excelTable.route('/uploadPdfFiles', methods=['POST'])
def upload_pdf_files():
  print("uploadPdfFiles....")

  _base_dir = current_app.config['pdfBaseDir']

  upload_type = request.form.get('uploadType')
  # æ ¹æ“š uploadType å‹•æ…‹èª¿æ•´å„²å­˜ç›®éŒ„
  if upload_type == 'pdf1':
      _base_dir = _base_dir.replace('ç‰©æ–™æ¸…å–®', 'é ˜é€€æ–™å–®')
  if not os.path.exists(_base_dir):
    os.makedirs(_base_dir)
  print("å¯¦éš›å„²å­˜è·¯å¾‘:", _base_dir)

  files = request.files.getlist('files')  # multiple files

  if not files or len(files) == 0:
    return jsonify({'message': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'}), 400

  saved_files = []
  for file in files:
    print("file.filename:",file.filename)

    if not file.filename.endswith('.pdf'):
      return jsonify({'message': f'æª”æ¡ˆ {file.filename} ä¸æ˜¯ PDF'}), 400

    filename = file.filename
    filename = os.path.basename(filename)
    file_path = os.path.join(_base_dir, filename)

    if os.path.exists(file_path):
      filename = get_unique_filename(_base_dir, filename, "copy")
      file_path = os.path.join(_base_dir, filename)

    file.save(file_path)
    saved_files.append(filename)

  return jsonify({
    'message': f'{len(saved_files)} å€‹ PDF æª”æ¡ˆä¸Šå‚³æˆåŠŸ',
    'files': saved_files,
    'status': True
  })


# å…§éƒ¨ï¼šå¥—ç”¨ BOM å·®ç•°(åŸ apply_bom_diffs çš„æ ¸å¿ƒï¼›ä¸å°å¤–æˆ route)
def _apply_bom_diffs_tx(s, material: Material, ops: list):
    """
    ç›´æ¥æŠŠå·®ç•°ï¼ˆadd/update/removeï¼‰å¥—ç”¨åˆ° DBã€‚å›å‚³(çµæœæ‘˜è¦, material çš„æœ€æ–° BOM åˆ—è¡¨)ã€‚
    """
    # å…ˆæŠ“ç¾æœ‰ BOMï¼Œå»ºç«‹ç´¢å¼•
    existing = s.query(Bom).filter(Bom.material_id == material.id).all()
    by_key, by_mnum = {}, {}
    for b in existing:
        k = (str(b.seq_num).strip(), str(b.material_num).strip())
        by_key[k] = b
        by_mnum.setdefault(str(b.material_num).strip(), []).append(b)

    def _to_int(x, default=0):
        try:
            if x is None or (isinstance(x, str) and x.strip() == ""):
                return default
            return int(float(x))
        except Exception:
            return default

    def _recompute_non_qty(b: Bom):
        req = _to_int(b.req_qty, 0)
        picked = _to_int(b.pick_qty, 0)
        b.non_qty = max(req - picked, 0)

    results = {"added": [], "updated": [], "removed": []}

    for op in ops:
        action = (op.get("action") or "").strip().lower()
        seq_num = str(op.get("seq_num") or "").strip()
        mnum = str(op.get("material_num") or "").strip()
        if not mnum:
            continue

        target = by_key.get((seq_num, mnum))
        if not target and seq_num == "" and mnum in by_mnum and len(by_mnum[mnum]) == 1:
            target = by_mnum[mnum][0]

        if action == "add":
            comment = op.get("mtl_comment", op.get("material_comment", ""))
            qty = op.get("qty", op.get("req_qty", 0))
            start_date = op.get("start_date") or material.material_delivery_date
            receive = op.get("receive", True)

            new_bom = Bom(
                material_id=material.id,
                seq_num=seq_num or op.get("id") or "0",
                material_num=mnum,
                material_comment=str(comment or ""),
                req_qty=_to_int(qty, 0),
                pick_qty=0,
                lack_qty=0,
                non_qty=0,
                receive=bool(receive),
                start_date=str(start_date or ""),
            )
            _recompute_non_qty(new_bom)
            s.add(new_bom)
            s.flush()
            by_key[(str(new_bom.seq_num), mnum)] = new_bom
            by_mnum.setdefault(mnum, []).append(new_bom)
            results["added"].append(new_bom.get_dict())

        elif action == "update":
            if not target:
                results["updated"].append({"material_num": mnum, "seq_num": seq_num, "skipped": "not_found"})
                continue
            new_comment = op.get("mtl_comment_new", op.get("material_comment"))
            new_qty = op.get("qty_new", op.get("req_qty"))
            new_seq = op.get("seq_num_new")
            changed = False
            if new_comment is not None and str(target.material_comment) != str(new_comment):
                target.material_comment = str(new_comment); changed = True
            if new_qty is not None:
                v = _to_int(new_qty, target.req_qty or 0)
                if v != (target.req_qty or 0):
                    target.req_qty = v
                    _recompute_non_qty(target)
                    changed = True
            if new_seq is not None:
                ns = str(new_seq).strip()
                if ns and ns != str(target.seq_num):
                    old_key = (str(target.seq_num), mnum)
                    target.seq_num = ns
                    by_key.pop(old_key, None)
                    by_key[(ns, mnum)] = target
                    changed = True
            if changed:
                results["updated"].append(target.get_dict())
            else:
                results["updated"].append({"material_num": mnum, "seq_num": seq_num, "noop": True})

        elif action == "remove":
            if not target:
                candidates = by_mnum.get(mnum, [])
                if not candidates:
                    results["removed"].append({"material_num": mnum, "seq_num": seq_num, "skipped": "not_found"})
                    continue
                for b in list(candidates):
                    s.delete(b)
                    results["removed"].append({"id": b.id, "material_num": b.material_num, "seq_num": b.seq_num})
                    by_key.pop((str(b.seq_num), mnum), None)
                by_mnum.pop(mnum, None)
            else:
                s.delete(target)
                results["removed"].append({"id": target.id, "material_num": target.material_num, "seq_num": target.seq_num})
                by_key.pop((str(target.seq_num), mnum), None)
                if mnum in by_mnum:
                    by_mnum[mnum] = [b for b in by_mnum[mnum] if b.id != target.id]
                    if not by_mnum[mnum]:
                        by_mnum.pop(mnum, None)

    # å–å›æœ€æ–° BOM
    latest = s.query(Bom).filter(Bom.material_id == material.id).all()
    return results, [b.get_dict() for b in latest]

# å°è£ï¼šå¾ excel_modify è®€æª” â†’ ç”¢ç”Ÿå·®ç•°æ¸…å–®(æ²¿ç”¨ read_all_excel_files çš„æ¸…æ´—è¦å‰‡)
def _collect_bom_diffs_from_modify_dir(s, material: Material):
    base_in = os.path.abspath(current_app.config['baseDir'])            # e.g. ...\excel_in
    modify_dir = base_in.replace("_in", "_modify")                      # ...\excel_modify
    out_dir = base_in.replace("_in", "_out")                            # ...\excel_out
    sheet_bom = current_app.config.get('excel_bom_sheet') or 1          # ç³»çµ±æŒ‡å®šç´¢å¼• 1 = BOM

    os.makedirs(modify_dir, exist_ok=True)
    os.makedirs(out_dir, exist_ok=True)

    order_num = normalize_order_number(material.order_num)
    files = [f for f in os.listdir(modify_dir)
        if os.path.isfile(os.path.join(modify_dir, f)) and f.endswith(('.xlsx', '.xls'))]

    modify_ops = []
    would_process_files = []

    # å…ˆæŠ“ç¾æœ‰ BOM â†’ map
    existing_bom = s.query(Bom).filter(Bom.material_id == material.id).all()
    existing_map = {str(b.material_num).strip(): b for b in existing_bom}

    for fname in files:
        path = os.path.join(modify_dir, fname)
        base_no_copy = re.sub(r'_copy_\d+', '', fname)  # èˆ‡ read_all_excel_files åŒé‚è¼¯é˜²é‡è¤‡

        already = s.query(exists().where(ProcessedFile.file_name == base_no_copy)).scalar()
        if already:
            print(f"[excelModifyTable] æª”æ¡ˆå·²è™•ç†ï¼Œç•¥éï¼š{fname}")
            continue

        # è®€ BOM sheetï¼ˆåŒ read_all_excel_files çš„æ¸…æ´—ï¼‰
        try:
            #bom_df = pd.read_excel(path, sheet_name=sheet_bom).fillna('')
            #
            bom_df = pd.read_excel(path, sheet_name=sheet_bom)
            # æ–‡æœ¬æ¬„ä½å¯è£œç©ºå­—ä¸²
            for col in ['ç‰©æ–™', 'ç‰©æ–™èªªæ˜']:
              if col in bom_df.columns:
                bom_df[col] = bom_df[col].fillna('')
            # æ•¸å€¼æ¬„ä½è½‰æ•¸å­—
            for col in ['ç‰©æ–™çŸ­ç¼º', 'éœ€æ±‚æ•¸é‡', 'é ç•™é …ç›®']:
              if col in bom_df.columns:
                bom_df[col] = pd.to_numeric(bom_df[col], errors='coerce').fillna(0).astype(int)
            #
        except Exception:
            # æŸäº›èˆŠæª”ç”¨ openpyxl å…ˆé©— sheet å­˜åœ¨
            with open(path, 'rb') as f:
                wb = openpyxl.load_workbook(filename=f, read_only=True)
                if isinstance(sheet_bom, str):
                    if sheet_bom not in wb.sheetnames:
                        print(f"[excelModifyTable] ç¼ºå°‘å·¥ä½œè¡¨ï¼š{sheet_bom}ï¼Œç•¥é {fname}")
                        continue
                else:
                    # å¦‚æœç”¨ç´¢å¼•ï¼Œé€™æ”¯åˆ†æ”¯ä¸åšï¼›ç›´æ¥ raise
                    raise
            bom_df = pd.read_excel(path, sheet_name=sheet_bom).fillna('')

        if 'è¨‚å–®' not in bom_df.columns:
            print(f"[excelModifyTable] ç¼ºå°‘ã€è¨‚å–®ã€æ¬„ï¼Œç•¥é {fname}")
            continue

        # è¨‚å–®æ¬„æ­£è¦åŒ–ï¼ˆæ¯”ç…§ read_all_excel_filesï¼‰
        bom_df.iloc[:, 0] = (
            bom_df.iloc[:, 0]
            .apply(normalize_order_number)
            .replace('nan', '')
            .astype(str)
        )

        rows = bom_df[bom_df['è¨‚å–®'] == order_num]
        if rows.empty:
            print(f"[excelModifyTable] æª”æ¡ˆ {fname} ç„¡æ­¤å–®è™Ÿ {order_num}ï¼Œç•¥é")
            continue

        # Excel(æ–°) â†’ map
        incoming_map = {}
        for _, r in rows.iterrows():
            mnum = str(clean_nan(r.get('ç‰©æ–™')) or '').strip()
            if not mnum:
                continue
            incoming_map[mnum] = {
                'seq_num':          clean_nan(r.get('é ç•™é …ç›®')),
                'material_comment': clean_nan(r.get('ç‰©æ–™èªªæ˜')) or '',
                'req_qty':          clean_nan(r.get('éœ€æ±‚æ•¸é‡')) or 0,
            }

        # remove
        for mnum, old in existing_map.items():
            if mnum not in incoming_map:
                modify_ops.append({
                    'action':      'remove',
                    'material_id': material.id,
                    'seq_num':     old.seq_num,
                    'material_num': mnum,
                })

        # add / update
        for mnum, new in incoming_map.items():
            old = existing_map.get(mnum)
            if not old:
                modify_ops.append({
                    'action':      'add',
                    'material_id': material.id,
                    'seq_num':     new['seq_num'],
                    'material_num': mnum,
                    'mtl_comment': new['material_comment'],
                    'qty':         new['req_qty'],
                    'start_date':  material.material_delivery_date,
                })
            else:
                changed = False
                old_c = str(old.material_comment or '')
                new_c = str(new['material_comment'] or '')
                if old_c != new_c:
                    changed = True
                try:
                    old_q = float(old.req_qty or 0)
                    new_q = float(new['req_qty'] or 0)
                    if old_q != new_q:
                        changed = True
                except Exception:
                    changed = True

                if changed:
                    modify_ops.append({
                        'action':          'update',
                        'material_id':     material.id,
                        'seq_num':         new['seq_num'] if new['seq_num'] is not None else old.seq_num,
                        'material_num':    mnum,
                        'mtl_comment_new': new['material_comment'],
                        'qty_new':         new['req_qty'],
                    })

        would_process_files.append((fname, path, base_no_copy, out_dir))

    return modify_ops, would_process_files


@excelTable.route('/modifyExcelFiles', methods=['POST'])
def modify_excel_files():
    print("modifyExcelFiles...")

    """
    è®€å– excel_modify å…§çš„ Excel â†’ ç”¢ç”Ÿ BOM å·®ç•° â†’ ï¼ˆé è¨­ï¼‰ç›´æ¥å¯«å…¥ DB â†’ ç§»æª”åˆ° excel_outã€‚
    åƒæ•¸ï¼š
      id / material_id: å·¥å–®ç·¨è™Ÿ
      dry_run: true å‰‡åªæ¯”å°ä¸å¯«åº«ã€ä¸ç§»æª”ï¼ˆé è¨­ Falseï¼‰
    å›å‚³ï¼š
      status, message, results(added/updated/removed), bom(æœ€æ–°), processedFiles(å¯¦éš›ç§»æª”æ¸…å–®)
    """
    payload = request.get_json(silent=True) or {}
    material_id = payload.get("material_id") or payload.get("id")
    #id = payload.get("id")
    dry_run = bool(payload.get("dry_run", False))

    print("material_id:",material_id)

    #if not material_id or not id:
    if not material_id:
        return jsonify({"status": False, "message": "ç¼ºå°‘ material_id / id"}), 400

    s = Session()
    try:
        material = s.query(Material).get(int(material_id))
        if not material:
            return jsonify({"status": False, "message": "æ‰¾ä¸åˆ°å°æ‡‰çš„ Material"}), 404

        # 1) æ”¶é›†å·®ç•°ï¼ˆä¸æ”¹ DBï¼‰
        # results:ã€Œå·®ç•°ã€æ˜ç´°ï¼Œè®“å‰ç«¯çŸ¥é“é€™æ¬¡åˆ°åº•å‹•äº†å“ªäº›è³‡æ–™
        #   results.added: Bom[]ï¼šæ–°å¢çš„ BOM åˆ—è¡¨ï¼ˆæ¯ä¸€ç­†é€šå¸¸æ˜¯ Bom.get_dict() çš„çµæœï¼‰ã€‚
        #   results.updated: (Bom | Meta)[]ï¼šæ›´æ–°éçš„ BOMã€‚è‹¥æŸç­†æ²’æœ‰ä»»ä½•è®ŠåŒ–ï¼Œæœƒå› "noop": trueï¼›è‹¥æ‰¾ä¸åˆ°å°æ‡‰èˆŠè³‡æ–™ï¼Œæœƒå› "skipped": "not_found"ã€‚
        #   results.removed: Meta[]ï¼šè¢«åˆªé™¤çš„ BOMï¼ˆé€šå¸¸åŒ…å« id / material_num / seq_num ç­‰è­˜åˆ¥æ¬„ä½ï¼‰ã€‚
        modify_ops, files_to_move = _collect_bom_diffs_from_modify_dir(s, material)
        if not modify_ops:
          return jsonify({
            "status": False,
            "message": "æ²’æœ‰å·®ç•°æˆ–æ²’æœ‰å¯è™•ç†çš„æª”æ¡ˆ(è«‹ä¸Šå‚³ä¿®æ­£å·¥å–®)",
            "results": {"added": [], "updated": [], "removed": []},
            "bom": [b.get_dict() for b in s.query(Bom).filter(Bom.material_id == material.id).all()],
            "processedFiles": []
          })

        if dry_run:
          # åƒ…æ¯”å°ï¼Œä¸å¯«åº«ã€ä¸ç§»æª”ã€ä¸è¨˜ ProcessedFile
          return jsonify({
            "status": True,
            "message": "dry_run åƒ…æ¯”å°å®Œæˆï¼Œæœªå¯«å…¥è³‡æ–™åº«ã€æœªç§»æª”",
            "diff_ops": modify_ops
          })

        # 2) å¯«å…¥ DBï¼ˆapply diffsï¼‰
        results, latest_bom = _apply_bom_diffs_tx(s, material, modify_ops)

        # 3) è¨˜éŒ„å·²è™•ç†ä¸¦ç§»æª”åˆ° excel_outï¼ˆå°¾ç¢¼ mdfï¼‰
        base_in = os.path.abspath(current_app.config['baseDir'])
        out_dir = base_in.replace("_in", "_out")
        processed = []
        for fname, path, base_no_copy, out_dir in files_to_move:
            pf = ProcessedFile(file_name=base_no_copy)
            s.add(pf)
            s.flush()

            new_name = get_unique_filename(out_dir, fname, "mdf")
            os.makedirs(out_dir, exist_ok=True)
            shutil.move(path, os.path.join(out_dir, new_name))
            processed.append(new_name)

        s.commit()

        return jsonify({
          "status": True,
          "message": "BOM å·®ç•°å·²å¥—ç”¨ä¸¦å®Œæˆç§»æª”",
          "results": results,
          "bom": latest_bom,
          "processedFiles": processed
        })

    except Exception as e:
        s.rollback()
        return jsonify({"status": False, "message": str(e)}), 500
    finally:
        try:
            s.close()
        except:
            pass

